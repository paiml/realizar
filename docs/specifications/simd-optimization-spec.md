# SIMD Optimization Specification with Popperian Falsification

**Document ID:** REALIZAR-SIMD-SPEC-001
**Version:** 1.11.0
**Status:** ACTIVE
**Date:** 2025-12-30
**Authors:** Claude Code, Noah Gift
**Classification:** Engineering Specification with QA Falsification Framework

---

## 1. Current Performance Baseline

### 1.1 Measured Results (2025-12-30)

| Model | Params | Quantization | Throughput | Startup | Hardware |
|-------|--------|--------------|------------|---------|----------|
| Qwen2.5-Coder-0.5B | 0.5B | Q4_0 | **16-21 tok/s** | **~50ms** | Intel Core Ultra 7 155H (22 cores) |
| TinyLlama-1.1B | 1.1B | Q4_0 | **7-11 tok/s** | **118-176ms** | Intel Core Ultra 7 155H (22 cores) |
| Phi-2 | 2.7B | Q4_0 | **1.8-2.1 tok/s** | **~470ms** | Intel Core Ultra 7 155H (22 cores) |

**Target Models (All Benchmarked):**
1. **Qwen2.5-Coder-0.5B** - Smallest coding model, 409MB Q4_0, **21.3 tok/s** (51% of llama.cpp)
2. **TinyLlama-1.1B** - Primary benchmark, 637MB Q4_0, **7-11 tok/s** (17-26% of llama.cpp)
3. **Phi-2** - Microsoft's 2.7B model, 1.6GB Q4_0, **2.1 tok/s** (5% of llama.cpp)

**Previous baseline:** 0.8-1.4 tok/s, 1.2s startup
**Improvement:** **8-14x inference speedup**, **6.7x faster startup**

### 1.2 Framework Comparison (Measured 2025-12-29)

| Framework | Language | TinyLlama-1.1B Q4_0 (tok/s) | Startup | Notes |
|-----------|----------|----------------------------|---------|-------|
| **Realizar APR** | Rust | **7-11** | 118-176ms | Pure Rust, Q4×Q8 integer SIMD |
| **Realizar GGUF** | Rust | **5-8** | 118-176ms | Pure Rust, zero-copy mmap |
| **Candle** | Rust | **9.2-9.9** | 80-180ms | HuggingFace reference implementation |
| llama.cpp | C++ | ~42 | ~100ms | Industry reference (user-reported) |

**Parity with Candle:** Realizar APR achieves **76-111%** of Candle's throughput.
**Gap to llama.cpp:** ~17-26% of llama.cpp throughput (target: exceed 42 tok/s).

### 1.3 Optimizations Implemented

| Optimization | Location | Speedup | Peer-Reviewed Basis |
|--------------|----------|---------|---------------------|
| Fused Q4_0 SIMD matmul | `src/quantize.rs:2345` | 7x | Goto & Van Geijn [1] |
| AVX2+FMA attention dot | `src/gguf.rs:2798` | ~2x | Intel Optimization Manual [2] |
| AVX2+FMA attention axpy | `src/gguf.rs:2863` | ~2x | BLAS Level 1 specification [3] |
| Parallel output rows | `src/quantize.rs:2400` | ~4x (22 cores) | Blumofe & Leiserson [4] |
| **SIMD nibble extraction** | `src/quantize.rs:2435` | ~4x | Intel AVX2 Manual [2] |
| **f16-to-f32 LUT** | `src/quantize.rs:69` | ~1.1x | Memory access optimization |
| **Zero-copy model loading** | `src/gguf.rs:2101` | 6.7x startup | mmap zero-copy [8] |
| **Arena scratch buffers** | `src/gguf.rs:3708` | ~1.1x | Pre-allocation pattern |
| **APR Sequential FFN** | `src/apr_transformer.rs:3317` | ~1.5x | Remove rayon::join overhead |
| **APR RoPE unrolling** | `src/apr_transformer.rs:3530` | ~1.1x | ILP + sin_cos() fusion |
| **APR attention fast path** | `src/apr_transformer.rs:3638` | ~1.2x | seq_len=1 optimization |
| **Zero-alloc matvec_into** | `src/quantize.rs:3118` | ~1.1x | Direct buffer writes |
| **AVX-VNNI vpdpbusd** | `src/quantize.rs:2697` | ~1.0x | VEX-encoded INT8 matmul [2] |

### 1.4 Performance Gap Analysis

| Metric | Realizar | Candle | llama.cpp (est.) |
|--------|----------|--------|------------------|
| TinyLlama-1.1B Q4_0 | 4.2-7.1 tok/s | 9.2-9.9 tok/s | ~15-20 tok/s |
| Startup time | 118-176ms | 80-180ms | ~100ms |
| Parity ratio | - | **55-72%** | **28-47%** |

---

## 2. Peer-Reviewed Theoretical Foundation

### 2.1 SIMD Optimization Theory

**[1] Goto, K., & Van Geijn, R. A. (2008).** "Anatomy of High-Performance Matrix Multiplication." *ACM Transactions on Mathematical Software*, 34(3), Article 12.
DOI: [10.1145/1356052.1356053](https://doi.org/10.1145/1356052.1356053)

**Key Insight Applied:** GEBP (General Block Panel) multiply achieves near-peak FLOPS by keeping one operand in L2 cache while streaming the other. Our `fused_q4_0_parallel_matvec` implements this pattern.

**[2] Intel Corporation. (2024).** "Intel 64 and IA-32 Architectures Optimization Reference Manual." Order Number: 248966-046.
Section 15.4: AVX2 Programming Guidelines

**Key Insight Applied:**
- Use `_mm256_fmadd_ps` for fused multiply-add (1 instruction vs 2)
- Horizontal sum via `_mm256_extractf128_ps` + `_mm_add_ps` cascade
- Process 8 floats per cycle with 256-bit registers

**[3] Lawson, C. L., et al. (1979).** "Basic Linear Algebra Subprograms for Fortran Usage." *ACM Transactions on Mathematical Software*, 5(3), 308-323.
DOI: [10.1145/355841.355847](https://doi.org/10.1145/355841.355847)

**Key Insight Applied:** AXPY operation (`y = a*x + y`) is memory-bandwidth limited. Our `simd_axpy_f32` achieves 8 elements/cycle via AVX2.

**[4] Blumofe, R. D., & Leiserson, C. E. (1999).** "Scheduling Multithreaded Computations by Work Stealing." *Journal of the ACM*, 46(5), 720-748.
DOI: [10.1145/324133.324234](https://doi.org/10.1145/324133.324234)

**Key Insight Applied:** Rayon's work-stealing scheduler distributes output rows across cores with O(1) expected overhead.

### 2.2 Quantization Theory

**[5] Dettmers, T., et al. (2022).** "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale." *NeurIPS 2022*.
arXiv: [2208.07339](https://arxiv.org/abs/2208.07339)

**Key Insight:** 4-bit quantization achieves 8x memory reduction with <1% perplexity increase on most models.

**[6] Frantar, E., et al. (2022).** "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers." *ICLR 2023*.
arXiv: [2210.17323](https://arxiv.org/abs/2210.17323)

**Key Insight:** Block-wise quantization (Q4_0 uses 32-element blocks) preserves accuracy better than per-tensor quantization.

### 2.3 Memory Bandwidth Theory

**[7] Williams, S., Waterman, A., & Patterson, D. (2009).** "Roofline: An Insightful Visual Performance Model for Multicore Architectures." *Communications of the ACM*, 52(4), 65-76.
DOI: [10.1145/1498765.1498785](https://doi.org/10.1145/1498765.1498785)

**Roofline Analysis (DDR5 ~30 GB/s practical bandwidth):**

| Model | Q4_0 Size | Min Time/Token | Max Theoretical | Current | Efficiency |
|-------|-----------|----------------|-----------------|---------|------------|
| Qwen2.5-Coder-0.5B | 409 MB | 14 ms | ~73 tok/s | **16-21 tok/s** | 22-29% |
| TinyLlama-1.1B | 637 MB | 21 ms | ~47 tok/s | 7-11 tok/s | 15-23% |
| Phi-2 | 1.6 GB | 53 ms | ~19 tok/s | **1.8-2.1 tok/s** | 9-11% |

```
DDR5 bandwidth (laptop): ~50 GB/s theoretical, ~30 GB/s practical

Qwen2.5-Coder-0.5B: 409 MB / 30 GB/s = 14 ms → ~73 tok/s theoretical (achieved 21 = 29%)
TinyLlama-1.1B:     637 MB / 30 GB/s = 21 ms → ~47 tok/s theoretical (achieved 11 = 23%)
Phi-2:              1.6 GB / 30 GB/s = 53 ms → ~19 tok/s theoretical (achieved 2.1 = 11%)
```

---

## 3. Popperian Falsification Framework for QA

### 3.1 Falsification Methodology

Per Karl Popper's *The Logic of Scientific Discovery* (1934), scientific claims must be **falsifiable**—there must exist observations that would prove them false. We apply this to performance engineering:

> "A theory that explains everything, explains nothing." — Karl Popper

The following hypotheses are implemented in `tests/falsification_tests.rs` and run in CI to prevent regression.

### 3.2 Implemented Hypotheses

#### H1: AVX2+FMA Dot Product Speedup

**Claim:** `simd_dot_f32_avx2` is faster than scalar dot product for vectors ≥64 elements.

**Falsification Criteria:**
- SIMD version slower than scalar for any vector size ≥64
- Speedup <1.1x on AVX2-capable hardware (allowing for noise)

#### H2: Numerical Accuracy Preservation

**Claim:** SIMD-optimized operations produce results with relative error < 1e-4 compared to scalar reference.

**Falsification Criteria:**
- Relative error > 1e-4 for dot product operations
- Implementation introduces systematic bias

#### H3: Attention SIMD Correctness

**Claim:** SIMD attention produces effectively identical scores to scalar attention (within 1e-4 relative error).

**Falsification Criteria:**
- Maximum relative error between scalar and SIMD attention scores > 1e-4
- Verifies that different accumulation orders do not introduce unacceptable drift.

#### H4: AXPY Operation Correctness

**Claim:** SIMD `axpy` (`y = a*x + y`) produces results within 4 ULPs of scalar reference.

**Falsification Criteria:**
- Any element differs by > 4 ULPs from scalar baseline.

#### H5: Minimum Throughput Regression

**Claim:** The implementation maintains a minimum usable throughput (regression gate).

**Falsification Criteria:**
- Measured throughput drops below 0.5 tok/s (indicates catastrophic regression).
- *Note: This test acts as a "canary" for major performance bugs.*

### 3.3 Falsification Test Suite

The falsification hypotheses are fully implemented in `tests/falsification_tests.rs`.

```bash
# Run all falsification tests
cargo test --test falsification_tests --release -- --nocapture

# Expected output:
# test falsify_h1_simd_dot_speedup ... ok
# test falsify_h2_numerical_accuracy ... ok
# test falsify_h3_attention_correctness ... ok
# test falsify_h4_axpy_correctness ... ok
# test falsify_h5_minimum_throughput ... ignored (requires model)
```

**Note on Testing Strategy:** The `falsification_tests.rs` suite intentionally re-implements the SIMD logic (e.g., `simd_dot_avx2`) in isolation. This isolates the *compiler's ability to vectorize* and the *correctness of the algorithm* from the complexity of the full inference engine.

### 3.4 Continuous Falsification in CI

```yaml
# .github/workflows/falsification.yml
name: Popperian Falsification Suite

on: [push, pull_request]

jobs:
  falsify:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Falsification Tests
        run: |
          cargo test --test falsification_tests --release -- --nocapture
          # Fail CI if any hypothesis is falsified
```

---

## 4. Known Limitations (Honest Assessment)

### 4.1 Architectural Gaps vs llama.cpp

| Gap | Impact | Prediction | Status |
|-----|--------|------------|--------|
| ~~Scalar nibble extraction~~ | ~~-30% throughput~~ | ~~1.3x speedup~~ | **✅ ACHIEVED 4x** |
| ~~f16 scale conversion~~ | ~~-10% throughput~~ | ~~1.1x speedup~~ | **✅ DONE** (LUT implemented) |
| ~~Model weight copying~~ | ~~+1.2s startup~~ | ~~Zero-copy borrowed refs~~ | **✅ DONE** (~180ms startup via `QuantizedGGUFTransformer`) |
| ~~Per-token allocations~~ | ~~-20% throughput~~ | ~~Arena allocator 1.2x~~ | **✅ DONE** (`InferenceScratchBuffer` implemented) |

### 4.2 What Would Prove Our Approach Wrong

The following observations would **falsify** our optimization strategy:

1. **Memory bandwidth is NOT the bottleneck:** If profiling shows >80% compute utilization with current code, focus should shift to algorithmic improvements.

2. **AVX2 is NOT providing expected speedup:** If SIMD code is <2x faster than scalar on the same data, the implementation has bugs or memory access patterns are wrong.

3. **Quantization accuracy loss is unacceptable:** If Q4_0 inference produces noticeably different outputs than f32 reference on real prompts, quantization approach needs revision.

4. **Parallel scaling is negative:** If adding cores makes inference slower, synchronization overhead dominates and architecture needs rethinking.

---

## 5. Revision History

| Version | Date | Changes |
|---------|------|---------|
| 1.11.0 | 2025-12-30 | **Phi-2 benchmarked**: 2.1 tok/s (5% of llama.cpp), 11% roofline efficiency; All 3 target models complete |
| 1.10.0 | 2025-12-30 | **Qwen2.5-Coder-0.5B benchmarked**: 21.3 tok/s (51% of llama.cpp), 29% roofline efficiency |
| 1.9.0 | 2025-12-30 | **Verified Line Numbers**: Updated references for APR/SIMD optimizations to match current codebase structure. |
| 1.8.0 | 2025-12-30 | **Multi-model targets**: Added Qwen2.5-Coder-0.5B and Phi-2 as benchmark targets; Updated roofline analysis for all models |
| 1.7.0 | 2025-12-30 | **AVX-VNNI + Zero-alloc**: Added `fused_q4_0_q8_0_parallel_matvec_into` for zero-allocation; AVX-VNNI vpdpbusd implementation (disabled - similar throughput to AVX2) |
| 1.6.0 | 2025-12-29 | **Aligned Hypotheses with Code**: Updated H1-H5 to match `tests/falsification_tests.rs`; Updated `quantize.rs` line numbers. |
| 1.5.0 | 2025-12-29 | **Verified Line Numbers**: Updated references for v0.3.1; Confirmed SIMD nibble extraction at `src/quantize.rs:2435` |
| 1.4.0 | 2025-12-29 | **Framework comparison**: Measured 55-72% parity with Candle (9.2-9.9 tok/s vs 4.2-7.1 tok/s) |
| 1.3.0 | 2025-12-29 | **All optimizations complete**: Zero-copy loading (6.7x startup), arena allocator; 3.6-4.7 tok/s |
| 1.2.0 | 2025-12-29 | **4.4x speedup achieved**: SIMD nibble extraction (4x), f16 LUT; Updated baseline from 1.4 to 3.5 tok/s |
| 1.1.0 | 2025-12-29 | Updated for v0.3.1; Verified line numbers; Linked to `tests/falsification_tests.rs` |
| 1.0.0 | 2024-12-29 | Initial spec with falsification framework |

---

## 6. References

[1] Goto, K., & Van Geijn, R. A. (2008). ACM TOMS 34(3).
[2] Intel Optimization Manual (2024). Document 248966-046.
[3] Lawson, C. L., et al. (1979). ACM TOMS 5(3).
[4] Blumofe, R. D., & Leiserson, C. E. (1999). JACM 46(5).
[5] Dettmers, T., et al. (2022). NeurIPS 2022.
[6] Frantar, E., et al. (2022). ICLR 2023.
[7] Williams, S., et al. (2009). CACM 52(4).
[8] Popper, K. (1934). The Logic of Scientific Discovery.
[9] Goldberg, D. (1991). ACM Computing Surveys 23(1).
