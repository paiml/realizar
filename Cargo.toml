[package]
name = "realizar"
version = "0.5.2"
edition = "2021"
rust-version = "1.89"
authors = ["Pragmatic AI Labs <contact@paiml.com>"]
license = "MIT"
description = "Pure Rust ML inference engine built from scratch - model serving for GGUF and safetensors"
repository = "https://github.com/paiml/realizar"
homepage = "https://github.com/paiml/realizar"
documentation = "https://docs.rs/realizar"
readme = "README.md"
keywords = ["machine-learning", "inference", "model-serving", "gguf", "transformer"]
categories = ["science", "web-programming::http-server"]

[lints.rust]
unsafe_op_in_unsafe_fn = "warn"
# unreachable_pub disabled - too noisy for test code in #[cfg(test)] modules
# unreachable_pub = "warn"
# Allow coverage cfg for cargo-llvm-cov
unexpected_cfgs = { level = "warn", check-cfg = ['cfg(coverage)'] }

[lints.clippy]
checked_conversions = "warn"
redundant_clone = "warn"
# Note: unwrap_used is enforced via .clippy.toml disallowed-methods instead
# Allow noisy lints in test code
similar_names = "allow"
unreadable_literal = "allow"
useless_vec = "allow"

[dependencies]
# OUR ecosystem - SIMD/GPU compute primitives (we control this)
# High-performance SIMD compute library with GPU support
# PAR-073: Path dependency for BrickProfiler development
trueno = { path = "../trueno", features = ["gpu"] }
# CUDA PTX generation and runtime for NVIDIA GPUs (optional, pure Rust, no LLVM/nvcc)
# Using path dependency for development - publish requires crates.io version
trueno-gpu = { version = "0.4.5", path = "../trueno/trueno-gpu", optional = true, features = ["cuda"] }
# KV cache storage (for attention caching)
trueno-db = { version = "0.3.8", optional = true }

# ML algorithms and .apr model format (optional for aprender model serving)
# v0.23: chat templates, GGUF tokenizer preservation
aprender = { version = "0.24", optional = true }

# Data loading library (optional for data pipeline examples)
alimentar = { version = "0.2", optional = true, default-features = false, features = ["local", "shuffle"] }

# Model registry (optional for pull/push commands)
pacha = { version = "0.2", optional = true }

# Terminal/PNG visualization for benchmarks and metrics
trueno-viz = { version = "0.1.6", optional = true, features = ["terminal"] }

# HTTP server ONLY (swappable via HttpServer trait)
axum = { version = "0.8.8", optional = true }
tokio = { version = "1", features = ["rt-multi-thread", "macros"], optional = true }
tokio-stream = { version = "0.1", optional = true }
tower = { version = "0.5.3", features = ["util"], optional = true }
futures = { version = "0.3", optional = true }
async-stream = { version = "0.3", optional = true }

# CLI
clap = { version = "4", features = ["derive"], optional = true }

# Lambda runtime (lightweight HTTP client)
ureq = { version = "3.1.4", features = ["json"], optional = true }

# TUI monitoring (PARITY-107)
ratatui = { version = "0.30.0", optional = true }
crossterm = { version = "0.29.0", optional = true }

# HTTP client for real model server benchmarking (blocking for bench harness)
reqwest = { version = "0.13.1", features = ["json", "blocking"], optional = true }

# Serialization (for REST API, not ML code)
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Lock-free concurrent data structures
arc-swap = { version = "1.7", optional = true }

# Error handling
thiserror = "2.0.17"
anyhow = "1.0"

# Structured logging / tracing (optional, for CUDA kernel spans)
tracing = { version = "0.1", optional = true }

# Audit logging (timestamps and UUIDs)
chrono = { version = "0.4.43", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }

# Numerical traits (for generic code)
num-traits = "0.2"

# Half-precision floats (for Q4_K and other K-quantization formats)
half = { version = "2.4", features = ["std"] }

# Lazy initialization (for f16-to-f32 LUT, per spec ยง4.1)
once_cell = "1.21"

# Memory-mapped file I/O for zero-copy model loading (per spec Phase 1)
memmap2 = "0.9"

# Compression for APR v2 format (GH-35)
lz4_flex = { version = "0.12.0", optional = true }
zstd = { version = "0.13", optional = true }

# Parallel processing for multi-core inference (per spec Phase 2/3)
rayon = "1.10"

# Random number generation for sampling
rand = "0.9.2"

# Small buffer optimization (per spec Section 4.1)
# Stack allocation for small token buffers and attention scores
smallvec = "1.13"

# Chat template engine (Jinja2-compatible) for model-specific formatting
# Supports ChatML, LLaMA2, Mistral, Phi, Alpaca formats
minijinja = { version = "2.14", features = ["loader"] }

# Embedding model support (BERT, Nomic, all-MiniLM) via candle
# Only enabled with "embeddings" feature - adds candle for transformer inference
candle-core = { version = "0.9.1", optional = true }
candle-nn = { version = "0.9.1", optional = true }
candle-transformers = { version = "0.9.1", optional = true }
tokenizers = { version = "0.22.2", optional = true }

# Note: Core GGUF/APR inference still uses our own implementation
# Candle is only used for dedicated embedding models (BERT, Nomic, etc.)

# System interface for memory pinning (mlock)
[target.'cfg(unix)'.dependencies]
libc = "0.2"

# WASM-specific: uuid needs js feature for browser RNG
[target.'cfg(target_arch = "wasm32")'.dependencies]
uuid = { version = "1", features = ["v4", "serde", "js"] }

[dev-dependencies]
# Visual regression testing framework (playbooks, TUI testing, GPU pixel verification)
jugar-probar = { version = "1.0.1", features = ["tui", "gpu"] }

# Benchmarking
criterion = { version = "0.8.1", features = ["html_reports"] }

# Serial test execution for GPU tests (CUDA context conflicts)
serial_test = "3"

# Rich terminal output for examples (progress bars, colors, tables)
indicatif = "0.18.3"
console = "0.16.2"
comfy-table = "7.2.2"

# Arrow for data pipeline example
arrow = { version = "57.2.0", default-features = false }

# Property-based testing
proptest = "1.4"

# Test utilities
approx = "0.5"

# For testing HTTP endpoints (canonical tower/axum testing)
http-body-util = "0.1"
hyper = { version = "1.8.1", features = ["full"] }
mime = "0.3"

# For external HTTP benchmarking (REAL model server calls)
reqwest = { version = "0.13.1", features = ["json", "blocking"] }

# For CLI integration tests
assert_cmd = "2.0"
predicates = "3.0"
glob = "0.3"

[features]
default = ["server", "cli", "gpu"]

# Core features
gpu = ["trueno/gpu"]  # Enable GPU acceleration via Trueno (wgpu)
cuda = ["dep:trueno-gpu", "dep:tracing"]  # Enable CUDA PTX generation for NVIDIA GPUs
kv-cache = ["dep:trueno-db"]  # KV cache for attention optimization
apr-compression = ["dep:lz4_flex", "dep:zstd"]  # LZ4/ZSTD decompression for APR v2 (GH-35)

# Optional components
server = ["dep:axum", "dep:tokio", "dep:tokio-stream", "dep:tower", "dep:futures", "dep:async-stream", "dep:arc-swap"]  # HTTP server support
cli = ["dep:clap", "server"]  # CLI binary (requires server)
aprender-serve = ["dep:aprender", "server"]  # Aprender ML model serving (requires server)
lambda = ["dep:ureq"]  # AWS Lambda handler (lightweight HTTP runtime)
alimentar-data = ["dep:alimentar"]  # Data loading with alimentar
visualization = ["dep:trueno-viz"]  # Terminal/PNG benchmark visualization
registry = ["dep:pacha"]  # Model registry with Pacha (pull/push/cache)
tui = ["dep:ratatui", "dep:crossterm", "dep:ureq", "dep:clap"]  # PARITY-107: Real-time monitoring TUI
bench-http = ["dep:reqwest"]  # Real HTTP benchmarking of external model servers
embeddings = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:tokenizers"]  # Embedding model support (BERT, Nomic, all-MiniLM)

# Testing features
load-test-enabled = ["server"]  # Enable load tests (requires running server)
heavy-tests = []  # Include 29k lines of gguf.rs tests (OOMs on <16GB RAM)

# Convenience meta-features
full = ["server", "cli", "gpu", "cuda", "registry"]  # Everything enabled
minimal = []  # Only core inference engine

[[bin]]
name = "realizar"
required-features = ["cli"]

[[bin]]
name = "wine_lambda"
required-features = ["lambda"]

[[bin]]
name = "mnist_lambda"
required-features = ["aprender-serve", "lambda"]

[[bin]]
name = "realizar-monitor"
path = "src/bin/realizar_monitor.rs"
required-features = ["tui"]

[[bench]]
name = "tensor_ops"
harness = false

[[bench]]
name = "inference"
harness = false

[[bench]]
name = "cache"
harness = false

[[bench]]
name = "tokenizer"
harness = false

[[bench]]
name = "quantize"
harness = false

[[bench]]
name = "lambda"
harness = false
required-features = ["lambda"]

[[bench]]
name = "comparative"
harness = false

[[bench]]
name = "apr_real"
harness = false

[[bench]]
name = "gguf_real"
harness = false

[[bench]]
name = "external_matrix"
harness = false

[[bench]]
name = "performance_parity"
harness = false

[[bench]]
name = "cuda_executor"
harness = false
required-features = ["cuda"]

[[example]]
name = "build_mnist_model"
required-features = ["aprender-serve"]

[[example]]
name = "serve_mnist"
required-features = ["aprender-serve"]

[[example]]
name = "mnist_apr_benchmark"
required-features = ["aprender-serve"]

[[example]]
name = "train_model"
required-features = ["aprender-serve"]

[[example]]
name = "data_pipeline"
required-features = ["alimentar-data"]

[[example]]
name = "performance_parity"

[[example]]
name = "chat_template"

[[example]]
name = "inference"

[[example]]
name = "tokenization"

[[example]]
name = "gguf_loading"

[[example]]
name = "cuda_debug"
required-features = ["cuda"]

[[example]]
name = "imp800_gpu_parity"
required-features = ["cuda"]

[[example]]
name = "imp900_optimized_gpu"
required-features = ["cuda"]

[[example]]
name = "imp_801_flash_attention_falsification"
required-features = ["cuda"]

[[example]]
name = "imp_1010_full_cuda_benchmark"
required-features = ["cuda"]

[[example]]
name = "debug_ptx"
required-features = ["cuda"]

[[example]]
name = "bench_gemv"
required-features = ["cuda"]

[[example]]
name = "test_gemv_correctness"
required-features = ["cuda"]

[[example]]
name = "parity_036_gpu_attention"
required-features = ["cuda"]

[[example]]
name = "parity_038_async_streams"
required-features = ["cuda"]

[[example]]
name = "parity_039_flash_attention"
required-features = ["cuda"]

[[example]]
name = "parity_040_fp16_attention"
required-features = ["cuda"]

[[example]]
name = "test_q4k_cuda"
required-features = ["cuda"]

[profile.release]
opt-level = 3
lto = "fat"           # Link-time optimization for maximum performance
codegen-units = 1     # Single codegen unit for max optimization
panic = "abort"       # Smaller binary size
strip = true          # Remove debug symbols

[profile.bench]
inherits = "release"

[profile.dev]
opt-level = 0         # Fast compilation
debug = true

# Faster compile times for development
[profile.dev.package."*"]
opt-level = 3

# Test profile - optimized for GPU benchmarks (PAR-023)
# Note: opt-level=3 required for accurate GPU timing measurements
[profile.test]
opt-level = 3
debug = false          # Reduce memory during debug info generation
lto = false            # Disable LTO for faster compile
codegen-units = 4      # Balance between speed and optimization
incremental = true     # Enable incremental compilation
