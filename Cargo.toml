[package]
name = "realizar"
version = "0.2.0"
edition = "2021"
rust-version = "1.75"
authors = ["Pragmatic AI Labs <contact@paiml.com>"]
license = "MIT"
description = "Pure Rust ML inference engine built from scratch - model serving for GGUF and safetensors"
repository = "https://github.com/paiml/realizar"
keywords = ["machine-learning", "inference", "model-serving", "gguf", "transformer"]
categories = ["science", "web-programming::http-server"]

[dependencies]
# OUR ecosystem - SIMD/GPU compute primitives (we control this)
trueno = { path = "../trueno", version = "0.2.2", features = ["gpu"] }

# HTTP server ONLY (swappable via HttpServer trait)
axum = { version = "0.7", optional = true }
tokio = { version = "1", features = ["rt-multi-thread", "macros"], optional = true }
tokio-stream = { version = "0.1", optional = true }
tower = { version = "0.4", features = ["util"], optional = true }
futures = { version = "0.3", optional = true }
async-stream = { version = "0.3", optional = true }

# CLI
clap = { version = "4", features = ["derive"], optional = true }

# Serialization (for REST API, not ML code)
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Numerical traits (for generic code)
num-traits = "0.2"

# That's it. NO candle, NO llama-cpp-rs, NO hf-hub
# We build everything from scratch: GGUF parser, safetensors, transformer, quantization, tokenizer

[dev-dependencies]
# Benchmarking
criterion = { version = "0.5", features = ["html_reports"] }

# Property-based testing
proptest = "1.4"

# Test utilities
approx = "0.5"

# For testing HTTP endpoints (Phase 1)
reqwest = { version = "0.11", features = ["json"] }

# For CLI integration tests
assert_cmd = "2.0"
predicates = "3.0"

[features]
default = ["server", "cli", "gpu"]

# Core features
gpu = ["trueno/gpu"]  # Enable GPU acceleration via Trueno

# Optional components
server = ["dep:axum", "dep:tokio", "dep:tokio-stream", "dep:tower", "dep:futures", "dep:async-stream"]  # HTTP server support
cli = ["dep:clap", "server"]  # CLI binary (requires server)

# Convenience meta-features
full = ["server", "cli", "gpu"]  # Everything enabled
minimal = []  # Only core inference engine

[[bin]]
name = "realizar"
required-features = ["cli"]

[[bench]]
name = "tensor_ops"
harness = false

[[bench]]
name = "inference"
harness = false

[[bench]]
name = "cache"
harness = false

[profile.release]
opt-level = 3
lto = "fat"           # Link-time optimization for maximum performance
codegen-units = 1     # Single codegen unit for max optimization
panic = "abort"       # Smaller binary size
strip = true          # Remove debug symbols

[profile.bench]
inherits = "release"

[profile.dev]
opt-level = 0         # Fast compilation
debug = true

# Faster compile times for development
[profile.dev.package."*"]
opt-level = 3
