[package]
name = "realizar"
version = "0.2.3"
edition = "2021"
rust-version = "1.75"
authors = ["Pragmatic AI Labs <contact@paiml.com>"]
license = "MIT"
description = "Pure Rust ML inference engine built from scratch - model serving for GGUF and safetensors"
repository = "https://github.com/paiml/realizar"
homepage = "https://github.com/paiml/realizar"
documentation = "https://docs.rs/realizar"
readme = "README.md"
keywords = ["machine-learning", "inference", "model-serving", "gguf", "transformer"]
categories = ["science", "web-programming::http-server"]

[lints.rust]
unsafe_op_in_unsafe_fn = "warn"
unreachable_pub = "warn"

[lints.clippy]
checked_conversions = "warn"
redundant_clone = "warn"
# Note: unwrap_used is enforced via .clippy.toml disallowed-methods instead

[dependencies]
# OUR ecosystem - SIMD/GPU compute primitives (we control this)
trueno = { version = "0.7.4", features = ["gpu"] }

# ML algorithms and .apr model format (optional for aprender model serving)
# v0.14: format module with .apr save/load, CRC32, signatures, encryption
aprender = { version = "0.14", optional = true }

# Data loading library (optional for data pipeline examples)
alimentar = { version = "0.2", optional = true, default-features = false, features = ["local", "shuffle"] }

# Model registry (optional for pull/push commands)
pacha = { version = "0.1.2", optional = true }

# Terminal/PNG visualization for benchmarks and metrics
trueno-viz = { version = "0.1.2", optional = true, features = ["terminal"] }

# HTTP server ONLY (swappable via HttpServer trait)
axum = { version = "0.7", optional = true }
tokio = { version = "1", features = ["rt-multi-thread", "macros"], optional = true }
tokio-stream = { version = "0.1", optional = true }
tower = { version = "0.4", features = ["util"], optional = true }
futures = { version = "0.3", optional = true }
async-stream = { version = "0.3", optional = true }

# CLI
clap = { version = "4", features = ["derive"], optional = true }

# Lambda runtime (lightweight HTTP client)
ureq = { version = "2", optional = true }

# Serialization (for REST API, not ML code)
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Lock-free concurrent data structures
arc-swap = { version = "1.7", optional = true }

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# System interface for memory pinning (mlock)
[target.'cfg(unix)'.dependencies]
libc = "0.2"

# Numerical traits (for generic code)
num-traits = "0.2"

# Half-precision floats (for Q4_K and other K-quantization formats)
half = { version = "2.4", features = ["std"] }

# That's it. NO candle, NO llama-cpp-rs, NO hf-hub
# We build everything from scratch: GGUF parser, safetensors, transformer, quantization, tokenizer

[dev-dependencies]
# Benchmarking
criterion = { version = "0.5", features = ["html_reports"] }

# Arrow for data pipeline example
arrow = { version = "53", default-features = false }

# Property-based testing
proptest = "1.4"

# Test utilities
approx = "0.5"

# For testing HTTP endpoints (canonical tower/axum testing)
http-body-util = "0.1"
hyper = { version = "1.4", features = ["full"] }
mime = "0.3"

# For external HTTP testing (integration tests)
reqwest = { version = "0.11", features = ["json"] }

# For CLI integration tests
assert_cmd = "2.0"
predicates = "3.0"

[features]
default = ["server", "cli", "gpu"]

# Core features
gpu = ["trueno/gpu"]  # Enable GPU acceleration via Trueno

# Optional components
server = ["dep:axum", "dep:tokio", "dep:tokio-stream", "dep:tower", "dep:futures", "dep:async-stream", "dep:arc-swap"]  # HTTP server support
cli = ["dep:clap", "server"]  # CLI binary (requires server)
aprender-serve = ["dep:aprender", "server"]  # Aprender ML model serving (requires server)
lambda = ["dep:ureq"]  # AWS Lambda handler (lightweight HTTP runtime)
alimentar-data = ["dep:alimentar"]  # Data loading with alimentar
visualization = ["dep:trueno-viz"]  # Terminal/PNG benchmark visualization
registry = ["dep:pacha"]  # Model registry with Pacha (pull/push/cache)

# Testing features
load-test-enabled = ["server"]  # Enable load tests (requires running server)

# Convenience meta-features
full = ["server", "cli", "gpu", "registry"]  # Everything enabled
minimal = []  # Only core inference engine

[[bin]]
name = "realizar"
required-features = ["cli"]

[[bin]]
name = "wine_lambda"
required-features = ["lambda"]

[[bin]]
name = "mnist_lambda"
required-features = ["aprender-serve", "lambda"]

[[bench]]
name = "tensor_ops"
harness = false

[[bench]]
name = "inference"
harness = false

[[bench]]
name = "cache"
harness = false

[[bench]]
name = "tokenizer"
harness = false

[[bench]]
name = "quantize"
harness = false

[[bench]]
name = "lambda"
harness = false
required-features = ["lambda"]

[[bench]]
name = "comparative"
harness = false

[[example]]
name = "build_mnist_model"
required-features = ["aprender-serve"]

[[example]]
name = "serve_mnist"
required-features = ["aprender-serve"]

[[example]]
name = "mnist_apr_benchmark"
required-features = ["aprender-serve"]

[[example]]
name = "train_model"
required-features = ["aprender-serve"]

[[example]]
name = "data_pipeline"
required-features = ["alimentar-data"]

[profile.release]
opt-level = 3
lto = "fat"           # Link-time optimization for maximum performance
codegen-units = 1     # Single codegen unit for max optimization
panic = "abort"       # Smaller binary size
strip = true          # Remove debug symbols

[profile.bench]
inherits = "release"

[profile.dev]
opt-level = 0         # Fast compilation
debug = true

# Faster compile times for development
[profile.dev.package."*"]
opt-level = 3
