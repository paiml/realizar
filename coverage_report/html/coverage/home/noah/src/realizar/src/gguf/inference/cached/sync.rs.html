<!doctype html><html><head><meta name='viewport' content='width=device-width,initial-scale=1'><meta charset='UTF-8'><link rel='stylesheet' type='text/css' href='../../../../../../../../../style.css'><script src='../../../../../../../../../control.js'></script></head><body><h2>Coverage Report</h2><h4>Created: 2026-01-25 15:05</h4><span class='control'><a href='javascript:next_line()'>next uncovered line (L)</a>, <a href='javascript:next_region()'>next uncovered region (R)</a>, <a href='javascript:next_branch()'>next uncovered branch (B)</a></span><div class='centered'><table><div class='source-name-title'><pre>/home/noah/src/realizar/src/gguf/inference/cached/sync.rs</pre></div><tr><td><pre>Line</pre></td><td><pre>Count</pre></td><td><pre>Source</pre></td></tr><tr><td class='line-number'><a name='L1' href='#L1'><pre>1</pre></a></td><td class='skipped-line'></td><td class='code'><pre>//! Thread-safe cached model wrapper (Mutex-based)</pre></td></tr><tr><td class='line-number'><a name='L2' href='#L2'><pre>2</pre></a></td><td class='skipped-line'></td><td class='code'><pre>//!</pre></td></tr><tr><td class='line-number'><a name='L3' href='#L3'><pre>3</pre></a></td><td class='skipped-line'></td><td class='code'><pre>//! `OwnedQuantizedModelCachedSync` uses Mutex for interior mutability,</pre></td></tr><tr><td class='line-number'><a name='L4' href='#L4'><pre>4</pre></a></td><td class='skipped-line'></td><td class='code'><pre>//! suitable for async HTTP servers and multi-threaded inference.</pre></td></tr><tr><td class='line-number'><a name='L5' href='#L5'><pre>5</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L6' href='#L6'><pre>6</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use crate::error::{RealizarError, Result};</pre></td></tr><tr><td class='line-number'><a name='L7' href='#L7'><pre>7</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use crate::gguf::{</pre></td></tr><tr><td class='line-number'><a name='L8' href='#L8'><pre>8</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    BatchGenerationStats, DispatchMetrics, OwnedQuantizedKVCache,</pre></td></tr><tr><td class='line-number'><a name='L9' href='#L9'><pre>9</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    OwnedQuantizedModel, QuantizedGenerateConfig,</pre></td></tr><tr><td class='line-number'><a name='L10' href='#L10'><pre>10</pre></a></td><td class='skipped-line'></td><td class='code'><pre>};</pre></td></tr><tr><td class='line-number'><a name='L11' href='#L11'><pre>11</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use super::weights::{DequantizedFFNWeights, DequantizedWeightCache};</pre></td></tr><tr><td class='line-number'><a name='L12' href='#L12'><pre>12</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L13' href='#L13'><pre>13</pre></a></td><td class='skipped-line'></td><td class='code'><pre>/// Thread-safe cached model wrapper with Mutex-based scheduler caching</pre></td></tr><tr><td class='line-number'><a name='L14' href='#L14'><pre>14</pre></a></td><td class='skipped-line'></td><td class='code'><pre>///</pre></td></tr><tr><td class='line-number'><a name='L15' href='#L15'><pre>15</pre></a></td><td class='skipped-line'></td><td class='code'><pre>/// Uses `Mutex` for interior mutability to cache GPU schedulers. Safe for</pre></td></tr><tr><td class='line-number'><a name='L16' href='#L16'><pre>16</pre></a></td><td class='skipped-line'></td><td class='code'><pre>/// multi-threaded HTTP serving with async handlers.</pre></td></tr><tr><td class='line-number'><a name='L17' href='#L17'><pre>17</pre></a></td><td class='skipped-line'></td><td class='code'><pre>pub struct OwnedQuantizedModelCachedSync {</pre></td></tr><tr><td class='line-number'><a name='L18' href='#L18'><pre>18</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Inner model (not cached)</pre></td></tr><tr><td class='line-number'><a name='L19' href='#L19'><pre>19</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    model: OwnedQuantizedModel,</pre></td></tr><tr><td class='line-number'><a name='L20' href='#L20'><pre>20</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Cached HybridScheduler for GPU operations (wgpu backend)</pre></td></tr><tr><td class='line-number'><a name='L21' href='#L21'><pre>21</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Uses Mutex for thread-safe interior mutability</pre></td></tr><tr><td class='line-number'><a name='L22' href='#L22'><pre>22</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    scheduler: std::sync::Mutex&lt;Option&lt;crate::gpu::HybridScheduler&gt;&gt;,</pre></td></tr><tr><td class='line-number'><a name='L23' href='#L23'><pre>23</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Cached CudaScheduler for direct CUDA operations</pre></td></tr><tr><td class='line-number'><a name='L24' href='#L24'><pre>24</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Bypasses wgpu 256MB buffer limit by using cuBLAS directly</pre></td></tr><tr><td class='line-number'><a name='L25' href='#L25'><pre>25</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;cuda&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L26' href='#L26'><pre>26</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    cuda_scheduler: std::sync::Mutex&lt;Option&lt;crate::gpu::CudaScheduler&gt;&gt;,</pre></td></tr><tr><td class='line-number'><a name='L27' href='#L27'><pre>27</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Dequantized weight cache for GPU batch inference (PARITY-019)</pre></td></tr><tr><td class='line-number'><a name='L28' href='#L28'><pre>28</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Uses RwLock for concurrent read access during batch inference</pre></td></tr><tr><td class='line-number'><a name='L29' href='#L29'><pre>29</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    dequant_cache: std::sync::RwLock&lt;Option&lt;DequantizedWeightCache&gt;&gt;,</pre></td></tr><tr><td class='line-number'><a name='L30' href='#L30'><pre>30</pre></a></td><td class='skipped-line'></td><td class='code'><pre>}</pre></td></tr><tr><td class='line-number'><a name='L31' href='#L31'><pre>31</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L32' href='#L32'><pre>32</pre></a></td><td class='skipped-line'></td><td class='code'><pre>// Explicitly implement Send + Sync for HTTP server usage</pre></td></tr><tr><td class='line-number'><a name='L33' href='#L33'><pre>33</pre></a></td><td class='skipped-line'></td><td class='code'><pre>#[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L34' href='#L34'><pre>34</pre></a></td><td class='skipped-line'></td><td class='code'><pre>unsafe impl Send for OwnedQuantizedModelCachedSync {}</pre></td></tr><tr><td class='line-number'><a name='L35' href='#L35'><pre>35</pre></a></td><td class='skipped-line'></td><td class='code'><pre>#[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L36' href='#L36'><pre>36</pre></a></td><td class='skipped-line'></td><td class='code'><pre>unsafe impl Sync for OwnedQuantizedModelCachedSync {}</pre></td></tr><tr><td class='line-number'><a name='L37' href='#L37'><pre>37</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L38' href='#L38'><pre>38</pre></a></td><td class='skipped-line'></td><td class='code'><pre>#[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L39' href='#L39'><pre>39</pre></a></td><td class='skipped-line'></td><td class='code'><pre>impl OwnedQuantizedModelCachedSync {</pre></td></tr><tr><td class='line-number'><a name='L40' href='#L40'><pre>40</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Create a new thread-safe cached model wrapper</pre></td></tr><tr><td class='line-number'><a name='L41' href='#L41'><pre>41</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L42' href='#L42'><pre>42</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// The scheduler is lazily initialized on first GPU operation.</pre></td></tr><tr><td class='line-number'><a name='L43' href='#L43'><pre>43</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// The dequantized weight cache is lazily initialized via `warmup_gpu_cache()`.</pre></td></tr><tr><td class='line-number'><a name='L44' href='#L44'><pre>44</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Also initializes CudaScheduler when CUDA feature is enabled.</pre></td></tr><tr><td class='line-number'><a name='L45' href='#L45'><pre>45</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[must_use]</pre></td></tr><tr><td class='line-number'><a name='L46' href='#L46'><pre>46</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>    pub fn new(model: OwnedQuantizedModel) -&gt; Self {</pre></td></tr><tr><td class='line-number'><a name='L47' href='#L47'><pre>47</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>        Self {</pre></td></tr><tr><td class='line-number'><a name='L48' href='#L48'><pre>48</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>            model,</pre></td></tr><tr><td class='line-number'><a name='L49' href='#L49'><pre>49</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>            scheduler: std::sync::Mutex::new(None),</pre></td></tr><tr><td class='line-number'><a name='L50' href='#L50'><pre>50</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>            #[cfg(feature = &quot;cuda&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L51' href='#L51'><pre>51</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>            cuda_scheduler: std::sync::Mutex::new(None),</pre></td></tr><tr><td class='line-number'><a name='L52' href='#L52'><pre>52</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>            dequant_cache: std::sync::RwLock::new(None),</pre></td></tr><tr><td class='line-number'><a name='L53' href='#L53'><pre>53</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L54' href='#L54'><pre>54</pre></a></td><td class='covered-line'><pre>39</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L55' href='#L55'><pre>55</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L56' href='#L56'><pre>56</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Get reference to inner model</pre></td></tr><tr><td class='line-number'><a name='L57' href='#L57'><pre>57</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[must_use]</pre></td></tr><tr><td class='line-number'><a name='L58' href='#L58'><pre>58</pre></a></td><td class='covered-line'><pre>2.64k</pre></td><td class='code'><pre>    pub fn model(&amp;self) -&gt; &amp;OwnedQuantizedModel {</pre></td></tr><tr><td class='line-number'><a name='L59' href='#L59'><pre>59</pre></a></td><td class='covered-line'><pre>2.64k</pre></td><td class='code'><pre>        &amp;self.model</pre></td></tr><tr><td class='line-number'><a name='L60' href='#L60'><pre>60</pre></a></td><td class='covered-line'><pre>2.64k</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L61' href='#L61'><pre>61</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L62' href='#L62'><pre>62</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Get or create the cached scheduler (thread-safe)</pre></td></tr><tr><td class='line-number'><a name='L63' href='#L63'><pre>63</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L64' href='#L64'><pre>64</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L65' href='#L65'><pre>65</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if scheduler creation fails or lock is poisoned</pre></td></tr><tr><td class='line-number'><a name='L66' href='#L66'><pre>66</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>fn get_scheduler(</span></pre></td></tr><tr><td class='line-number'><a name='L67' href='#L67'><pre>67</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L68' href='#L68'><pre>68</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;std::sync::MutexGuard&lt;&apos;_, Option&lt;crate::gpu::HybridScheduler&gt;&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L69' href='#L69'><pre>69</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut scheduler_opt</span> =</pre></td></tr><tr><td class='line-number'><a name='L70' href='#L70'><pre>70</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>self.scheduler</span></pre></td></tr><tr><td class='line-number'><a name='L71' href='#L71'><pre>71</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>lock</span>()</pre></td></tr><tr><td class='line-number'><a name='L72' href='#L72'><pre>72</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>map_err</span>(|_| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L73' href='#L73'><pre>73</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    operation: <span class='region red'>&quot;scheduler_lock&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L74' href='#L74'><pre>74</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    reason: <span class='region red'>&quot;Scheduler mutex poisoned&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L75' href='#L75'><pre>75</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L76' href='#L76'><pre>76</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L77' href='#L77'><pre>77</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Initialize if not already done</pre></td></tr><tr><td class='line-number'><a name='L78' href='#L78'><pre>78</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>scheduler_opt.is_none()</span> {</pre></td></tr><tr><td class='line-number'><a name='L79' href='#L79'><pre>79</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            use crate::gpu::HybridScheduler;</pre></td></tr><tr><td class='line-number'><a name='L80' href='#L80'><pre>80</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>new_scheduler</span> = <span class='region red'>HybridScheduler::with_threshold</span>(1000).<span class='region red'>map_err</span>(|e| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L81' href='#L81'><pre>81</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>RealizarError::UnsupportedOperation {</span></pre></td></tr><tr><td class='line-number'><a name='L82' href='#L82'><pre>82</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    operation: </span><span class='region red'>&quot;HybridScheduler::with_threshold&quot;</span><span class='region red'>.</span><span class='region red'>to_string</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L83' href='#L83'><pre>83</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    reason: format!(&quot;GPU scheduler initialization failed: {e}&quot;),</span></pre></td></tr><tr><td class='line-number'><a name='L84' href='#L84'><pre>84</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L85' href='#L85'><pre>85</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L86' href='#L86'><pre>86</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>*scheduler_opt</span> = <span class='region red'>Some(new_scheduler)</span>;</pre></td></tr><tr><td class='line-number'><a name='L87' href='#L87'><pre>87</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L88' href='#L88'><pre>88</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L89' href='#L89'><pre>89</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(scheduler_opt)</span></pre></td></tr><tr><td class='line-number'><a name='L90' href='#L90'><pre>90</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L91' href='#L91'><pre>91</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L92' href='#L92'><pre>92</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Get or create the cached CUDA scheduler (thread-safe)</pre></td></tr><tr><td class='line-number'><a name='L93' href='#L93'><pre>93</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L94' href='#L94'><pre>94</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Bypasses wgpu 256MB buffer limit by using cuBLAS directly.</pre></td></tr><tr><td class='line-number'><a name='L95' href='#L95'><pre>95</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns None if CUDA is not available.</pre></td></tr><tr><td class='line-number'><a name='L96' href='#L96'><pre>96</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L97' href='#L97'><pre>97</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L98' href='#L98'><pre>98</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if lock is poisoned</pre></td></tr><tr><td class='line-number'><a name='L99' href='#L99'><pre>99</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;cuda&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L100' href='#L100'><pre>100</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    fn get_cuda_scheduler(</pre></td></tr><tr><td class='line-number'><a name='L101' href='#L101'><pre>101</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L102' href='#L102'><pre>102</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ) -&gt; Result&lt;std::sync::MutexGuard&lt;&apos;_, Option&lt;crate::gpu::CudaScheduler&gt;&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L103' href='#L103'><pre>103</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        use crate::gpu::CudaScheduler;</pre></td></tr><tr><td class='line-number'><a name='L104' href='#L104'><pre>104</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L105' href='#L105'><pre>105</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        let mut scheduler_opt =</pre></td></tr><tr><td class='line-number'><a name='L106' href='#L106'><pre>106</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            self.cuda_scheduler</pre></td></tr><tr><td class='line-number'><a name='L107' href='#L107'><pre>107</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                .lock()</pre></td></tr><tr><td class='line-number'><a name='L108' href='#L108'><pre>108</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                .map_err(|_| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L109' href='#L109'><pre>109</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    operation: &quot;cuda_scheduler_lock&quot;.to_string(),</pre></td></tr><tr><td class='line-number'><a name='L110' href='#L110'><pre>110</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    reason: &quot;CUDA scheduler mutex poisoned&quot;.to_string(),</pre></td></tr><tr><td class='line-number'><a name='L111' href='#L111'><pre>111</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                })?;</pre></td></tr><tr><td class='line-number'><a name='L112' href='#L112'><pre>112</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L113' href='#L113'><pre>113</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Initialize if not already done</pre></td></tr><tr><td class='line-number'><a name='L114' href='#L114'><pre>114</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        if scheduler_opt.is_none() {</pre></td></tr><tr><td class='line-number'><a name='L115' href='#L115'><pre>115</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            match CudaScheduler::new() {</pre></td></tr><tr><td class='line-number'><a name='L116' href='#L116'><pre>116</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                Ok(new_scheduler) =&gt; {</pre></td></tr><tr><td class='line-number'><a name='L117' href='#L117'><pre>117</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    eprintln!(&quot;PARITY-103: CudaScheduler initialized successfully&quot;);</pre></td></tr><tr><td class='line-number'><a name='L118' href='#L118'><pre>118</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    *scheduler_opt = Some(new_scheduler);</pre></td></tr><tr><td class='line-number'><a name='L119' href='#L119'><pre>119</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                },</pre></td></tr><tr><td class='line-number'><a name='L120' href='#L120'><pre>120</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                Err(e) =&gt; {</pre></td></tr><tr><td class='line-number'><a name='L121' href='#L121'><pre>121</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // CUDA not available, leave as None (will fallback to wgpu)</pre></td></tr><tr><td class='line-number'><a name='L122' href='#L122'><pre>122</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    eprintln!(&quot;PARITY-103: CudaScheduler::new() failed: {:?}&quot;, e);</pre></td></tr><tr><td class='line-number'><a name='L123' href='#L123'><pre>123</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                },</pre></td></tr><tr><td class='line-number'><a name='L124' href='#L124'><pre>124</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L125' href='#L125'><pre>125</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L126' href='#L126'><pre>126</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L127' href='#L127'><pre>127</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        Ok(scheduler_opt)</pre></td></tr><tr><td class='line-number'><a name='L128' href='#L128'><pre>128</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L129' href='#L129'><pre>129</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L130' href='#L130'><pre>130</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Batch matmul preferring CUDA over wgpu (thread-safe)</pre></td></tr><tr><td class='line-number'><a name='L131' href='#L131'><pre>131</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L132' href='#L132'><pre>132</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Tries CudaScheduler first (no buffer limits), falls back to HybridScheduler (wgpu).</pre></td></tr><tr><td class='line-number'><a name='L133' href='#L133'><pre>133</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// This bypasses the wgpu 256MB buffer limit that was blocking GPU batch inference.</pre></td></tr><tr><td class='line-number'><a name='L134' href='#L134'><pre>134</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;cuda&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L135' href='#L135'><pre>135</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    fn batch_matmul_gpu_prefer_cuda(</pre></td></tr><tr><td class='line-number'><a name='L136' href='#L136'><pre>136</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L137' href='#L137'><pre>137</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        input: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L138' href='#L138'><pre>138</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        weight_f32: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L139' href='#L139'><pre>139</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        batch_size: usize,</pre></td></tr><tr><td class='line-number'><a name='L140' href='#L140'><pre>140</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        in_dim: usize,</pre></td></tr><tr><td class='line-number'><a name='L141' href='#L141'><pre>141</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        out_dim: usize,</pre></td></tr><tr><td class='line-number'><a name='L142' href='#L142'><pre>142</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L143' href='#L143'><pre>143</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Validate input</pre></td></tr><tr><td class='line-number'><a name='L144' href='#L144'><pre>144</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        if input.len() != batch_size * in_dim {</pre></td></tr><tr><td class='line-number'><a name='L145' href='#L145'><pre>145</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            return Err(RealizarError::InvalidShape {</pre></td></tr><tr><td class='line-number'><a name='L146' href='#L146'><pre>146</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                reason: format!(</pre></td></tr><tr><td class='line-number'><a name='L147' href='#L147'><pre>147</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    &quot;Input size {} doesn&apos;t match batch_size={} * in_dim={}&quot;,</pre></td></tr><tr><td class='line-number'><a name='L148' href='#L148'><pre>148</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    input.len(),</pre></td></tr><tr><td class='line-number'><a name='L149' href='#L149'><pre>149</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    batch_size,</pre></td></tr><tr><td class='line-number'><a name='L150' href='#L150'><pre>150</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    in_dim</pre></td></tr><tr><td class='line-number'><a name='L151' href='#L151'><pre>151</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                ),</pre></td></tr><tr><td class='line-number'><a name='L152' href='#L152'><pre>152</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            });</pre></td></tr><tr><td class='line-number'><a name='L153' href='#L153'><pre>153</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L154' href='#L154'><pre>154</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L155' href='#L155'><pre>155</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Try CUDA first (no buffer size limits)</pre></td></tr><tr><td class='line-number'><a name='L156' href='#L156'><pre>156</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        if let Ok(mut cuda_guard) = self.get_cuda_scheduler() {</pre></td></tr><tr><td class='line-number'><a name='L157' href='#L157'><pre>157</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            if let Some(ref mut cuda_sched) = *cuda_guard {</pre></td></tr><tr><td class='line-number'><a name='L158' href='#L158'><pre>158</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                return cuda_sched</pre></td></tr><tr><td class='line-number'><a name='L159' href='#L159'><pre>159</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    .matmul(input, weight_f32, batch_size, in_dim, out_dim)</pre></td></tr><tr><td class='line-number'><a name='L160' href='#L160'><pre>160</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    .map_err(|e| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L161' href='#L161'><pre>161</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        operation: &quot;batch_matmul_gpu_prefer_cuda&quot;.to_string(),</pre></td></tr><tr><td class='line-number'><a name='L162' href='#L162'><pre>162</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        reason: format!(&quot;CUDA matmul failed: {e}&quot;),</pre></td></tr><tr><td class='line-number'><a name='L163' href='#L163'><pre>163</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    });</pre></td></tr><tr><td class='line-number'><a name='L164' href='#L164'><pre>164</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L165' href='#L165'><pre>165</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L166' href='#L166'><pre>166</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L167' href='#L167'><pre>167</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Fallback to wgpu (may hit 256MB limit for large batches)</pre></td></tr><tr><td class='line-number'><a name='L168' href='#L168'><pre>168</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        let mut scheduler_guard = self.get_scheduler()?;</pre></td></tr><tr><td class='line-number'><a name='L169' href='#L169'><pre>169</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        if let Some(ref mut scheduler) = *scheduler_guard {</pre></td></tr><tr><td class='line-number'><a name='L170' href='#L170'><pre>170</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            return scheduler</pre></td></tr><tr><td class='line-number'><a name='L171' href='#L171'><pre>171</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                .matmul(input, weight_f32, batch_size, in_dim, out_dim)</pre></td></tr><tr><td class='line-number'><a name='L172' href='#L172'><pre>172</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                .map_err(|e| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L173' href='#L173'><pre>173</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    operation: &quot;batch_matmul_gpu_prefer_cuda&quot;.to_string(),</pre></td></tr><tr><td class='line-number'><a name='L174' href='#L174'><pre>174</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    reason: format!(&quot;GPU matmul failed: {e}&quot;),</pre></td></tr><tr><td class='line-number'><a name='L175' href='#L175'><pre>175</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                });</pre></td></tr><tr><td class='line-number'><a name='L176' href='#L176'><pre>176</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L177' href='#L177'><pre>177</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L178' href='#L178'><pre>178</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        Err(RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L179' href='#L179'><pre>179</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            operation: &quot;batch_matmul_gpu_prefer_cuda&quot;.to_string(),</pre></td></tr><tr><td class='line-number'><a name='L180' href='#L180'><pre>180</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            reason: &quot;No GPU scheduler available&quot;.to_string(),</pre></td></tr><tr><td class='line-number'><a name='L181' href='#L181'><pre>181</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        })</pre></td></tr><tr><td class='line-number'><a name='L182' href='#L182'><pre>182</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L183' href='#L183'><pre>183</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L184' href='#L184'><pre>184</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Batch matmul preferring CUDA (non-CUDA fallback)</pre></td></tr><tr><td class='line-number'><a name='L185' href='#L185'><pre>185</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(not(feature = &quot;cuda&quot;))]</pre></td></tr><tr><td class='line-number'><a name='L186' href='#L186'><pre>186</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>fn batch_matmul_gpu_prefer_cuda(</span></pre></td></tr><tr><td class='line-number'><a name='L187' href='#L187'><pre>187</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L188' href='#L188'><pre>188</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        input: &amp;[f32],</span></pre></td></tr><tr><td class='line-number'><a name='L189' href='#L189'><pre>189</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        weight_f32: &amp;[f32],</span></pre></td></tr><tr><td class='line-number'><a name='L190' href='#L190'><pre>190</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        batch_size: usize,</span></pre></td></tr><tr><td class='line-number'><a name='L191' href='#L191'><pre>191</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        in_dim: usize,</span></pre></td></tr><tr><td class='line-number'><a name='L192' href='#L192'><pre>192</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        out_dim: usize,</span></pre></td></tr><tr><td class='line-number'><a name='L193' href='#L193'><pre>193</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L194' href='#L194'><pre>194</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Validate input</pre></td></tr><tr><td class='line-number'><a name='L195' href='#L195'><pre>195</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>input</span><span class='region red'>.</span><span class='region red'>len</span><span class='region red'>() != batch_size * in_dim</span> {</pre></td></tr><tr><td class='line-number'><a name='L196' href='#L196'><pre>196</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Err(RealizarError::InvalidShape {</span></pre></td></tr><tr><td class='line-number'><a name='L197' href='#L197'><pre>197</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                reason: </span><span class='region red'>format!</span><span class='region red'>(</span></pre></td></tr><tr><td class='line-number'><a name='L198' href='#L198'><pre>198</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>&quot;Input size {} doesn&apos;t match batch_size={} * in_dim={}&quot;</span><span class='region red'>,</span></pre></td></tr><tr><td class='line-number'><a name='L199' href='#L199'><pre>199</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    input.len(),</span></pre></td></tr><tr><td class='line-number'><a name='L200' href='#L200'><pre>200</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    batch_size,</span></pre></td></tr><tr><td class='line-number'><a name='L201' href='#L201'><pre>201</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    in_dim</span></pre></td></tr><tr><td class='line-number'><a name='L202' href='#L202'><pre>202</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                ),</span></pre></td></tr><tr><td class='line-number'><a name='L203' href='#L203'><pre>203</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            })</span>;</pre></td></tr><tr><td class='line-number'><a name='L204' href='#L204'><pre>204</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L205' href='#L205'><pre>205</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L206' href='#L206'><pre>206</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut scheduler_guard</span> = <span class='region red'>self</span>.<span class='region red'>get_scheduler</span>()<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L207' href='#L207'><pre>207</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if let Some(<span class='region red'>ref mut scheduler</span>) = <span class='region red'>*scheduler_guard</span> {</pre></td></tr><tr><td class='line-number'><a name='L208' href='#L208'><pre>208</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>scheduler</span></pre></td></tr><tr><td class='line-number'><a name='L209' href='#L209'><pre>209</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>matmul</span>(<span class='region red'>input</span>, <span class='region red'>weight_f32</span>, <span class='region red'>batch_size</span>, <span class='region red'>in_dim</span>, <span class='region red'>out_dim</span>)</pre></td></tr><tr><td class='line-number'><a name='L210' href='#L210'><pre>210</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>map_err</span>(|e| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L211' href='#L211'><pre>211</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    operation: <span class='region red'>&quot;batch_matmul_gpu_prefer_cuda&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L212' href='#L212'><pre>212</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    reason: <span class='region red'>format!</span>(<span class='region red'>&quot;GPU matmul failed: {e}&quot;</span>),</pre></td></tr><tr><td class='line-number'><a name='L213' href='#L213'><pre>213</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>);</pre></td></tr><tr><td class='line-number'><a name='L214' href='#L214'><pre>214</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L215' href='#L215'><pre>215</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L216' href='#L216'><pre>216</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Err(RealizarError::UnsupportedOperation {</span></pre></td></tr><tr><td class='line-number'><a name='L217' href='#L217'><pre>217</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            operation: </span><span class='region red'>&quot;batch_matmul_gpu_prefer_cuda&quot;</span><span class='region red'>.</span><span class='region red'>to_string</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L218' href='#L218'><pre>218</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            reason: &quot;No GPU scheduler available&quot;.to_string(),</span></pre></td></tr><tr><td class='line-number'><a name='L219' href='#L219'><pre>219</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        })</span></pre></td></tr><tr><td class='line-number'><a name='L220' href='#L220'><pre>220</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L221' href='#L221'><pre>221</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L222' href='#L222'><pre>222</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Generate tokens with KV cache using thread-safe cached scheduler</pre></td></tr><tr><td class='line-number'><a name='L223' href='#L223'><pre>223</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L224' href='#L224'><pre>224</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Delegates to the inner model&apos;s `generate_with_cache` method.</pre></td></tr><tr><td class='line-number'><a name='L225' href='#L225'><pre>225</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// The scheduler caching benefits GPU batch operations; single-token</pre></td></tr><tr><td class='line-number'><a name='L226' href='#L226'><pre>226</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// generation uses CPU path with KV cache for O(n) scaling.</pre></td></tr><tr><td class='line-number'><a name='L227' href='#L227'><pre>227</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L228' href='#L228'><pre>228</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L229' href='#L229'><pre>229</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `prompt` - Input token IDs</pre></td></tr><tr><td class='line-number'><a name='L230' href='#L230'><pre>230</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `config` - Generation configuration</pre></td></tr><tr><td class='line-number'><a name='L231' href='#L231'><pre>231</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L232' href='#L232'><pre>232</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L233' href='#L233'><pre>233</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Generated token sequence including prompt</pre></td></tr><tr><td class='line-number'><a name='L234' href='#L234'><pre>234</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L235' href='#L235'><pre>235</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L236' href='#L236'><pre>236</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if generation fails</pre></td></tr><tr><td class='line-number'><a name='L237' href='#L237'><pre>237</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn generate_with_cache(</span></pre></td></tr><tr><td class='line-number'><a name='L238' href='#L238'><pre>238</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L239' href='#L239'><pre>239</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        prompt: &amp;[u32],</span></pre></td></tr><tr><td class='line-number'><a name='L240' href='#L240'><pre>240</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        config: &amp;QuantizedGenerateConfig,</span></pre></td></tr><tr><td class='line-number'><a name='L241' href='#L241'><pre>241</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;Vec&lt;u32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L242' href='#L242'><pre>242</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Delegate to inner model - CPU path with KV cache is already efficient</pre></td></tr><tr><td class='line-number'><a name='L243' href='#L243'><pre>243</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>self.model</span>.<span class='region red'>generate_with_cache</span>(<span class='region red'>prompt</span>, <span class='region red'>config</span>)</pre></td></tr><tr><td class='line-number'><a name='L244' href='#L244'><pre>244</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L245' href='#L245'><pre>245</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L246' href='#L246'><pre>246</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Generate tokens with adaptive CPU/GPU attention (IMP-126)</pre></td></tr><tr><td class='line-number'><a name='L247' href='#L247'><pre>247</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L248' href='#L248'><pre>248</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// This variant of `generate_with_cache` uses adaptive CPU/GPU dispatch</pre></td></tr><tr><td class='line-number'><a name='L249' href='#L249'><pre>249</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// based on cache length and records dispatch decisions to metrics.</pre></td></tr><tr><td class='line-number'><a name='L250' href='#L250'><pre>250</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L251' href='#L251'><pre>251</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L252' href='#L252'><pre>252</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `prompt` - Initial token IDs</pre></td></tr><tr><td class='line-number'><a name='L253' href='#L253'><pre>253</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `config` - Generation configuration</pre></td></tr><tr><td class='line-number'><a name='L254' href='#L254'><pre>254</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `metrics` - Dispatch metrics tracker for CPU/GPU decision recording</pre></td></tr><tr><td class='line-number'><a name='L255' href='#L255'><pre>255</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L256' href='#L256'><pre>256</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L257' href='#L257'><pre>257</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Generated token sequence including prompt</pre></td></tr><tr><td class='line-number'><a name='L258' href='#L258'><pre>258</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L259' href='#L259'><pre>259</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L260' href='#L260'><pre>260</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if generation fails</pre></td></tr><tr><td class='line-number'><a name='L261' href='#L261'><pre>261</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L262' href='#L262'><pre>262</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>    pub fn generate_with_cache_adaptive(</pre></td></tr><tr><td class='line-number'><a name='L263' href='#L263'><pre>263</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L264' href='#L264'><pre>264</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        prompt: &amp;[u32],</pre></td></tr><tr><td class='line-number'><a name='L265' href='#L265'><pre>265</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        config: &amp;QuantizedGenerateConfig,</pre></td></tr><tr><td class='line-number'><a name='L266' href='#L266'><pre>266</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        metrics: &amp;std::sync::Arc&lt;DispatchMetrics&gt;,</pre></td></tr><tr><td class='line-number'><a name='L267' href='#L267'><pre>267</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>    ) -&gt; Result&lt;Vec&lt;u32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L268' href='#L268'><pre>268</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Delegate to inner model&apos;s adaptive generation</pre></td></tr><tr><td class='line-number'><a name='L269' href='#L269'><pre>269</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        self.model</pre></td></tr><tr><td class='line-number'><a name='L270' href='#L270'><pre>270</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>            .generate_with_cache_adaptive(prompt, config, metrics)</pre></td></tr><tr><td class='line-number'><a name='L271' href='#L271'><pre>271</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L272' href='#L272'><pre>272</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L273' href='#L273'><pre>273</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Forward pass with cached scheduler (thread-safe)</pre></td></tr><tr><td class='line-number'><a name='L274' href='#L274'><pre>274</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L275' href='#L275'><pre>275</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Uses the cached HybridScheduler for GPU operations.</pre></td></tr><tr><td class='line-number'><a name='L276' href='#L276'><pre>276</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L277' href='#L277'><pre>277</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L278' href='#L278'><pre>278</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if GPU operations fail</pre></td></tr><tr><td class='line-number'><a name='L279' href='#L279'><pre>279</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[allow(clippy::let_underscore_untyped)] // Placeholder for future use</pre></td></tr><tr><td class='line-number'><a name='L280' href='#L280'><pre>280</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn forward_batch_gpu_cached(&amp;self, token_ids: &amp;[u32]) -&gt; Result&lt;Vec&lt;f32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L281' href='#L281'><pre>281</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>batch_size</span> = <span class='region red'>token_ids</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L282' href='#L282'><pre>282</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>vocab_size</span> = <span class='region red'>self.model.config.vocab_size</span>;</pre></td></tr><tr><td class='line-number'><a name='L283' href='#L283'><pre>283</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L284' href='#L284'><pre>284</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Get cached scheduler (for future GPU operations)</pre></td></tr><tr><td class='line-number'><a name='L285' href='#L285'><pre>285</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut scheduler_guard</span> = <span class='region red'>self</span>.<span class='region red'>get_scheduler</span>()<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L286' href='#L286'><pre>286</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let _ = <span class='region red'>scheduler_guard</span></pre></td></tr><tr><td class='line-number'><a name='L287' href='#L287'><pre>287</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            .as_mut()</span></pre></td></tr><tr><td class='line-number'><a name='L288' href='#L288'><pre>288</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>ok_or_else</span>(|| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L289' href='#L289'><pre>289</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                operation: <span class='region red'>&quot;forward_batch_gpu_cached&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L290' href='#L290'><pre>290</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                reason: <span class='region red'>&quot;Scheduler not initialized&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L291' href='#L291'><pre>291</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L292' href='#L292'><pre>292</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L293' href='#L293'><pre>293</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // 1. Token embedding lookup</pre></td></tr><tr><td class='line-number'><a name='L294' href='#L294'><pre>294</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden</span> = <span class='region red'>self.model</span>.<span class='region red'>embed</span>(<span class='region red'>token_ids</span>);</pre></td></tr><tr><td class='line-number'><a name='L295' href='#L295'><pre>295</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L296' href='#L296'><pre>296</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // 2. Process through layers</pre></td></tr><tr><td class='line-number'><a name='L297' href='#L297'><pre>297</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        for <span class='region red'>layer</span> in <span class='region red'>&amp;self.model.layers</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L298' href='#L298'><pre>298</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            // Simplified single-layer forward - reuse inner model logic</span></pre></td></tr><tr><td class='line-number'><a name='L299' href='#L299'><pre>299</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            // For full implementation, would need to port the complete forward pass</span></pre></td></tr><tr><td class='line-number'><a name='L300' href='#L300'><pre>300</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            let _ = layer;</span></pre></td></tr><tr><td class='line-number'><a name='L301' href='#L301'><pre>301</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        }</span></pre></td></tr><tr><td class='line-number'><a name='L302' href='#L302'><pre>302</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L303' href='#L303'><pre>303</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // 3. Output normalization and LM head</pre></td></tr><tr><td class='line-number'><a name='L304' href='#L304'><pre>304</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // For now, return placeholder - full implementation requires porting forward logic</pre></td></tr><tr><td class='line-number'><a name='L305' href='#L305'><pre>305</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>output</span> = <span class='region red'>vec!</span>[0.0f32; <span class='region red'>batch_size * vocab_size</span>];</pre></td></tr><tr><td class='line-number'><a name='L306' href='#L306'><pre>306</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let _ = <span class='region red'>hidden</span>;</pre></td></tr><tr><td class='line-number'><a name='L307' href='#L307'><pre>307</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L308' href='#L308'><pre>308</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(output)</span></pre></td></tr><tr><td class='line-number'><a name='L309' href='#L309'><pre>309</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L310' href='#L310'><pre>310</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L311' href='#L311'><pre>311</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Adaptive fused attention for production serving (IMP-121)</pre></td></tr><tr><td class='line-number'><a name='L312' href='#L312'><pre>312</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L313' href='#L313'><pre>313</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Thread-safe wrapper that automatically selects CPU or GPU based on</pre></td></tr><tr><td class='line-number'><a name='L314' href='#L314'><pre>314</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// sequence length. Uses the cached scheduler for efficient GPU operations.</pre></td></tr><tr><td class='line-number'><a name='L315' href='#L315'><pre>315</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L316' href='#L316'><pre>316</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L317' href='#L317'><pre>317</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `q` - Query tensor [seq_len, head_dim]</pre></td></tr><tr><td class='line-number'><a name='L318' href='#L318'><pre>318</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `k` - Key tensor [seq_len, head_dim]</pre></td></tr><tr><td class='line-number'><a name='L319' href='#L319'><pre>319</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `v` - Value tensor [seq_len, head_dim]</pre></td></tr><tr><td class='line-number'><a name='L320' href='#L320'><pre>320</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `seq_len` - Sequence length</pre></td></tr><tr><td class='line-number'><a name='L321' href='#L321'><pre>321</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `head_dim` - Head dimension</pre></td></tr><tr><td class='line-number'><a name='L322' href='#L322'><pre>322</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `scale` - Attention scale factor</pre></td></tr><tr><td class='line-number'><a name='L323' href='#L323'><pre>323</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L324' href='#L324'><pre>324</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L325' href='#L325'><pre>325</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Output tensor [seq_len, head_dim]</pre></td></tr><tr><td class='line-number'><a name='L326' href='#L326'><pre>326</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>    pub fn adaptive_fused_attention(</pre></td></tr><tr><td class='line-number'><a name='L327' href='#L327'><pre>327</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L328' href='#L328'><pre>328</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        q: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L329' href='#L329'><pre>329</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        k: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L330' href='#L330'><pre>330</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        v: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L331' href='#L331'><pre>331</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        seq_len: usize,</pre></td></tr><tr><td class='line-number'><a name='L332' href='#L332'><pre>332</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        head_dim: usize,</pre></td></tr><tr><td class='line-number'><a name='L333' href='#L333'><pre>333</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        scale: f32,</pre></td></tr><tr><td class='line-number'><a name='L334' href='#L334'><pre>334</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L335' href='#L335'><pre>335</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Threshold for GPU dispatch (from IMP-119 analysis)</pre></td></tr><tr><td class='line-number'><a name='L336' href='#L336'><pre>336</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        const GPU_SEQ_LEN_THRESHOLD: usize = 64;</pre></td></tr><tr><td class='line-number'><a name='L337' href='#L337'><pre>337</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L338' href='#L338'><pre>338</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>        if seq_len &gt;= GPU_SEQ_LEN_THRESHOLD {</pre></td></tr><tr><td class='line-number'><a name='L339' href='#L339'><pre>339</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // Long sequence: Use GPU path</pre></td></tr><tr><td class='line-number'><a name='L340' href='#L340'><pre>340</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            self.gpu_fused_causal_attention(q, k, v, seq_len, head_dim, scale)</pre></td></tr><tr><td class='line-number'><a name='L341' href='#L341'><pre>341</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        } else {</pre></td></tr><tr><td class='line-number'><a name='L342' href='#L342'><pre>342</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // Short sequence: Use CPU path</pre></td></tr><tr><td class='line-number'><a name='L343' href='#L343'><pre>343</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>            self.cpu_fused_causal_attention(q, k, v, seq_len, head_dim, scale)</pre></td></tr><tr><td class='line-number'><a name='L344' href='#L344'><pre>344</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L345' href='#L345'><pre>345</pre></a></td><td class='covered-line'><pre>9</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L346' href='#L346'><pre>346</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L347' href='#L347'><pre>347</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// CPU fused causal attention (thread-safe wrapper)</pre></td></tr><tr><td class='line-number'><a name='L348' href='#L348'><pre>348</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>    fn cpu_fused_causal_attention(</pre></td></tr><tr><td class='line-number'><a name='L349' href='#L349'><pre>349</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L350' href='#L350'><pre>350</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        q: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L351' href='#L351'><pre>351</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        k: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L352' href='#L352'><pre>352</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        v: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L353' href='#L353'><pre>353</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        seq_len: usize,</pre></td></tr><tr><td class='line-number'><a name='L354' href='#L354'><pre>354</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        head_dim: usize,</pre></td></tr><tr><td class='line-number'><a name='L355' href='#L355'><pre>355</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        scale: f32,</pre></td></tr><tr><td class='line-number'><a name='L356' href='#L356'><pre>356</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L357' href='#L357'><pre>357</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Use tiled implementation from inner model</pre></td></tr><tr><td class='line-number'><a name='L358' href='#L358'><pre>358</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>        self.model</pre></td></tr><tr><td class='line-number'><a name='L359' href='#L359'><pre>359</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>            .tiled_causal_attention(q, k, v, seq_len, head_dim, scale, 4)</pre></td></tr><tr><td class='line-number'><a name='L360' href='#L360'><pre>360</pre></a></td><td class='covered-line'><pre>5</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L361' href='#L361'><pre>361</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L362' href='#L362'><pre>362</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// GPU fused causal attention (thread-safe)</pre></td></tr><tr><td class='line-number'><a name='L363' href='#L363'><pre>363</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>    fn gpu_fused_causal_attention(</pre></td></tr><tr><td class='line-number'><a name='L364' href='#L364'><pre>364</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L365' href='#L365'><pre>365</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        q: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L366' href='#L366'><pre>366</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        k: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L367' href='#L367'><pre>367</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        v: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L368' href='#L368'><pre>368</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        seq_len: usize,</pre></td></tr><tr><td class='line-number'><a name='L369' href='#L369'><pre>369</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        head_dim: usize,</pre></td></tr><tr><td class='line-number'><a name='L370' href='#L370'><pre>370</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        scale: f32,</pre></td></tr><tr><td class='line-number'><a name='L371' href='#L371'><pre>371</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L372' href='#L372'><pre>372</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        let mut scheduler_guard =</pre></td></tr><tr><td class='line-number'><a name='L373' href='#L373'><pre>373</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            self.scheduler</pre></td></tr><tr><td class='line-number'><a name='L374' href='#L374'><pre>374</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>                .lock()</pre></td></tr><tr><td class='line-number'><a name='L375' href='#L375'><pre>375</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>                .map_err(|_| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L376' href='#L376'><pre>376</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    operation: <span class='region red'>&quot;gpu_fused_causal_attention&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L377' href='#L377'><pre>377</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    reason: <span class='region red'>&quot;Failed to acquire scheduler lock&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L378' href='#L378'><pre>378</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L379' href='#L379'><pre>379</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L380' href='#L380'><pre>380</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Initialize scheduler if needed</pre></td></tr><tr><td class='line-number'><a name='L381' href='#L381'><pre>381</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        if scheduler_guard.is_none() {</pre></td></tr><tr><td class='line-number'><a name='L382' href='#L382'><pre>382</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            use crate::gpu::HybridScheduler;</pre></td></tr><tr><td class='line-number'><a name='L383' href='#L383'><pre>383</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            let new_scheduler = HybridScheduler::with_threshold(1000).map_err(|e| <div class='tooltip'><span class='region red'>{</span><span class='tooltip-content'>0</span></div></pre></td></tr><tr><td class='line-number'><a name='L384' href='#L384'><pre>384</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>RealizarError::UnsupportedOperation {</span></pre></td></tr><tr><td class='line-number'><a name='L385' href='#L385'><pre>385</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    operation: </span><span class='region red'>&quot;HybridScheduler::with_threshold&quot;</span><span class='region red'>.</span><span class='region red'>to_string</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L386' href='#L386'><pre>386</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    reason: format!(&quot;GPU scheduler initialization failed: {e}&quot;),</span></pre></td></tr><tr><td class='line-number'><a name='L387' href='#L387'><pre>387</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L388' href='#L388'><pre>388</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L389' href='#L389'><pre>389</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            *scheduler_guard = Some(new_scheduler);</pre></td></tr><tr><td class='line-number'><a name='L390' href='#L390'><pre>390</pre></a></td><td class='covered-line'><pre>3</pre></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L391' href='#L391'><pre>391</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L392' href='#L392'><pre>392</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        let scheduler =</pre></td></tr><tr><td class='line-number'><a name='L393' href='#L393'><pre>393</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            scheduler_guard</pre></td></tr><tr><td class='line-number'><a name='L394' href='#L394'><pre>394</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>                .as_mut()</pre></td></tr><tr><td class='line-number'><a name='L395' href='#L395'><pre>395</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>                .ok_or_else(|| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L396' href='#L396'><pre>396</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    operation: <span class='region red'>&quot;gpu_fused_causal_attention&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L397' href='#L397'><pre>397</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    reason: <span class='region red'>&quot;Scheduler not initialized&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L398' href='#L398'><pre>398</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L399' href='#L399'><pre>399</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L400' href='#L400'><pre>400</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Transpose K for matmul</pre></td></tr><tr><td class='line-number'><a name='L401' href='#L401'><pre>401</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        let mut k_transposed = vec![0.0f32; head_dim * seq_len];</pre></td></tr><tr><td class='line-number'><a name='L402' href='#L402'><pre>402</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>        for pos in 0..<div class='tooltip'>seq_len<span class='tooltip-content'>4</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L403' href='#L403'><pre>403</pre></a></td><td class='covered-line'><pre>4.09k</pre></td><td class='code'><pre>            for d in 0..<div class='tooltip'>head_dim<span class='tooltip-content'>256</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L404' href='#L404'><pre>404</pre></a></td><td class='covered-line'><pre>4.09k</pre></td><td class='code'><pre>                k_transposed[d * seq_len + pos] = k[pos * head_dim + d];</pre></td></tr><tr><td class='line-number'><a name='L405' href='#L405'><pre>405</pre></a></td><td class='covered-line'><pre>4.09k</pre></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L406' href='#L406'><pre>406</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L407' href='#L407'><pre>407</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L408' href='#L408'><pre>408</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // GPU Q @ K^T</pre></td></tr><tr><td class='line-number'><a name='L409' href='#L409'><pre>409</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        let scores = scheduler</pre></td></tr><tr><td class='line-number'><a name='L410' href='#L410'><pre>410</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            .matmul(q, &amp;k_transposed, seq_len, head_dim, seq_len)</pre></td></tr><tr><td class='line-number'><a name='L411' href='#L411'><pre>411</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            .map_err(|e| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L412' href='#L412'><pre>412</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                operation: <span class='region red'>&quot;gpu_fused Q@K^T&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L413' href='#L413'><pre>413</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                reason: <span class='region red'>format!</span>(<span class='region red'>&quot;GPU matmul failed: {}&quot;</span>, e),</pre></td></tr><tr><td class='line-number'><a name='L414' href='#L414'><pre>414</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L415' href='#L415'><pre>415</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L416' href='#L416'><pre>416</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // CPU causal softmax</pre></td></tr><tr><td class='line-number'><a name='L417' href='#L417'><pre>417</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        let mut weights = vec![0.0f32; seq_len * seq_len];</pre></td></tr><tr><td class='line-number'><a name='L418' href='#L418'><pre>418</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>        for i in 0..<div class='tooltip'>seq_len<span class='tooltip-content'>4</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L419' href='#L419'><pre>419</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>            let mut max_val = f32::NEG_INFINITY;</pre></td></tr><tr><td class='line-number'><a name='L420' href='#L420'><pre>420</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>            for j in 0..=<div class='tooltip'>i<span class='tooltip-content'>256</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L421' href='#L421'><pre>421</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                let score = scores[i * seq_len + j] * scale;</pre></td></tr><tr><td class='line-number'><a name='L422' href='#L422'><pre>422</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                if score &gt; max_val {</pre></td></tr><tr><td class='line-number'><a name='L423' href='#L423'><pre>423</pre></a></td><td class='covered-line'><pre>1.09k</pre></td><td class='code'><pre>                    max_val = score;</pre></td></tr><tr><td class='line-number'><a name='L424' href='#L424'><pre>424</pre></a></td><td class='covered-line'><pre>7.22k</pre></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L425' href='#L425'><pre>425</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L426' href='#L426'><pre>426</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>            let mut sum = 0.0f32;</pre></td></tr><tr><td class='line-number'><a name='L427' href='#L427'><pre>427</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>            for j in 0..=<div class='tooltip'>i<span class='tooltip-content'>256</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L428' href='#L428'><pre>428</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                let score = scores[i * seq_len + j] * scale;</pre></td></tr><tr><td class='line-number'><a name='L429' href='#L429'><pre>429</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                weights[i * seq_len + j] = (score - max_val).exp();</pre></td></tr><tr><td class='line-number'><a name='L430' href='#L430'><pre>430</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                sum += weights[i * seq_len + j];</pre></td></tr><tr><td class='line-number'><a name='L431' href='#L431'><pre>431</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L432' href='#L432'><pre>432</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>            if sum &gt; 0.0 {</pre></td></tr><tr><td class='line-number'><a name='L433' href='#L433'><pre>433</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                for j in 0..=<div class='tooltip'>i<span class='tooltip-content'>256</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L434' href='#L434'><pre>434</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                    weights[i * seq_len + j] /= sum;</pre></td></tr><tr><td class='line-number'><a name='L435' href='#L435'><pre>435</pre></a></td><td class='covered-line'><pre>8.32k</pre></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L436' href='#L436'><pre>436</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L437' href='#L437'><pre>437</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L438' href='#L438'><pre>438</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L439' href='#L439'><pre>439</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // GPU weights @ V</pre></td></tr><tr><td class='line-number'><a name='L440' href='#L440'><pre>440</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        scheduler</pre></td></tr><tr><td class='line-number'><a name='L441' href='#L441'><pre>441</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            .matmul(&amp;weights, v, seq_len, seq_len, head_dim)</pre></td></tr><tr><td class='line-number'><a name='L442' href='#L442'><pre>442</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            .map_err(|e| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L443' href='#L443'><pre>443</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                operation: <span class='region red'>&quot;gpu_fused weights@V&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L444' href='#L444'><pre>444</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                reason: <span class='region red'>format!</span>(<span class='region red'>&quot;GPU matmul failed: {}&quot;</span>, e),</pre></td></tr><tr><td class='line-number'><a name='L445' href='#L445'><pre>445</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span>)</pre></td></tr><tr><td class='line-number'><a name='L446' href='#L446'><pre>446</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L447' href='#L447'><pre>447</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L448' href='#L448'><pre>448</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Adaptive multihead attention for production serving (IMP-121)</pre></td></tr><tr><td class='line-number'><a name='L449' href='#L449'><pre>449</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L450' href='#L450'><pre>450</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Thread-safe multi-head attention that automatically selects backend.</pre></td></tr><tr><td class='line-number'><a name='L451' href='#L451'><pre>451</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L452' href='#L452'><pre>452</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L453' href='#L453'><pre>453</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `q` - Query tensor [seq_len, hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L454' href='#L454'><pre>454</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `k` - Key tensor [seq_len, hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L455' href='#L455'><pre>455</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `v` - Value tensor [seq_len, hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L456' href='#L456'><pre>456</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `seq_len` - Sequence length</pre></td></tr><tr><td class='line-number'><a name='L457' href='#L457'><pre>457</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L458' href='#L458'><pre>458</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L459' href='#L459'><pre>459</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Output tensor [seq_len, hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L460' href='#L460'><pre>460</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    pub fn adaptive_multihead_attention(</pre></td></tr><tr><td class='line-number'><a name='L461' href='#L461'><pre>461</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;self,</pre></td></tr><tr><td class='line-number'><a name='L462' href='#L462'><pre>462</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        q: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L463' href='#L463'><pre>463</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        k: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L464' href='#L464'><pre>464</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        v: &amp;[f32],</pre></td></tr><tr><td class='line-number'><a name='L465' href='#L465'><pre>465</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        seq_len: usize,</pre></td></tr><tr><td class='line-number'><a name='L466' href='#L466'><pre>466</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L467' href='#L467'><pre>467</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let hidden_dim = self.model.config.hidden_dim;</pre></td></tr><tr><td class='line-number'><a name='L468' href='#L468'><pre>468</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let num_heads = self.model.config.num_heads;</pre></td></tr><tr><td class='line-number'><a name='L469' href='#L469'><pre>469</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let head_dim = hidden_dim / num_heads;</pre></td></tr><tr><td class='line-number'><a name='L470' href='#L470'><pre>470</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let scale = 1.0 / (head_dim as f32).sqrt();</pre></td></tr><tr><td class='line-number'><a name='L471' href='#L471'><pre>471</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L472' href='#L472'><pre>472</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Reshape Q, K, V to [num_heads, seq_len, head_dim]</pre></td></tr><tr><td class='line-number'><a name='L473' href='#L473'><pre>473</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let q_reshaped = self</pre></td></tr><tr><td class='line-number'><a name='L474' href='#L474'><pre>474</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            .model</pre></td></tr><tr><td class='line-number'><a name='L475' href='#L475'><pre>475</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            .reshape_for_parallel_heads(q, seq_len, num_heads, head_dim)<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L476' href='#L476'><pre>476</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let k_reshaped = self</pre></td></tr><tr><td class='line-number'><a name='L477' href='#L477'><pre>477</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            .model</pre></td></tr><tr><td class='line-number'><a name='L478' href='#L478'><pre>478</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            .reshape_for_parallel_heads(k, seq_len, num_heads, head_dim)<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L479' href='#L479'><pre>479</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let v_reshaped = self</pre></td></tr><tr><td class='line-number'><a name='L480' href='#L480'><pre>480</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            .model</pre></td></tr><tr><td class='line-number'><a name='L481' href='#L481'><pre>481</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>            .reshape_for_parallel_heads(v, seq_len, num_heads, head_dim)<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L482' href='#L482'><pre>482</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L483' href='#L483'><pre>483</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let mut attn_output = vec![0.0f32; num_heads * seq_len * head_dim];</pre></td></tr><tr><td class='line-number'><a name='L484' href='#L484'><pre>484</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L485' href='#L485'><pre>485</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        for h in 0..<div class='tooltip'>num_heads<span class='tooltip-content'>1</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L486' href='#L486'><pre>486</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            let head_offset = h * seq_len * head_dim;</pre></td></tr><tr><td class='line-number'><a name='L487' href='#L487'><pre>487</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            let q_head = &amp;q_reshaped[head_offset..head_offset + seq_len * head_dim];</pre></td></tr><tr><td class='line-number'><a name='L488' href='#L488'><pre>488</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            let k_head = &amp;k_reshaped[head_offset..head_offset + seq_len * head_dim];</pre></td></tr><tr><td class='line-number'><a name='L489' href='#L489'><pre>489</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            let v_head = &amp;v_reshaped[head_offset..head_offset + seq_len * head_dim];</pre></td></tr><tr><td class='line-number'><a name='L490' href='#L490'><pre>490</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L491' href='#L491'><pre>491</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            let head_output =</pre></td></tr><tr><td class='line-number'><a name='L492' href='#L492'><pre>492</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>                self.adaptive_fused_attention(q_head, k_head, v_head, seq_len, head_dim, scale)<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L493' href='#L493'><pre>493</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L494' href='#L494'><pre>494</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            attn_output[head_offset..head_offset + seq_len * head_dim]</pre></td></tr><tr><td class='line-number'><a name='L495' href='#L495'><pre>495</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>                .copy_from_slice(&amp;head_output);</pre></td></tr><tr><td class='line-number'><a name='L496' href='#L496'><pre>496</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L497' href='#L497'><pre>497</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L498' href='#L498'><pre>498</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Reshape back to [seq_len, hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L499' href='#L499'><pre>499</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        let mut output = vec![0.0f32; seq_len * hidden_dim];</pre></td></tr><tr><td class='line-number'><a name='L500' href='#L500'><pre>500</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>        for h in 0..<div class='tooltip'>num_heads<span class='tooltip-content'>1</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L501' href='#L501'><pre>501</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>            let head_start = h * seq_len * head_dim;</pre></td></tr><tr><td class='line-number'><a name='L502' href='#L502'><pre>502</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>            for pos in 0..<div class='tooltip'>seq_len<span class='tooltip-content'>4</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L503' href='#L503'><pre>503</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>                let src_start = head_start + pos * head_dim;</pre></td></tr><tr><td class='line-number'><a name='L504' href='#L504'><pre>504</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>                let dst_start = pos * hidden_dim + h * head_dim;</pre></td></tr><tr><td class='line-number'><a name='L505' href='#L505'><pre>505</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>                output[dst_start..dst_start + head_dim]</pre></td></tr><tr><td class='line-number'><a name='L506' href='#L506'><pre>506</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>                    .copy_from_slice(&amp;attn_output[src_start..src_start + head_dim]);</pre></td></tr><tr><td class='line-number'><a name='L507' href='#L507'><pre>507</pre></a></td><td class='covered-line'><pre>256</pre></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L508' href='#L508'><pre>508</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L509' href='#L509'><pre>509</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L510' href='#L510'><pre>510</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        Ok(output)</pre></td></tr><tr><td class='line-number'><a name='L511' href='#L511'><pre>511</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L512' href='#L512'><pre>512</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L513' href='#L513'><pre>513</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Warmup GPU weight cache for batch inference (PARITY-019)</pre></td></tr><tr><td class='line-number'><a name='L514' href='#L514'><pre>514</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L515' href='#L515'><pre>515</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Pre-dequantizes all FFN weights to f32 for GPU GEMM operations.</pre></td></tr><tr><td class='line-number'><a name='L516' href='#L516'><pre>516</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Call this once at server startup to avoid dequantization during inference.</pre></td></tr><tr><td class='line-number'><a name='L517' href='#L517'><pre>517</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L518' href='#L518'><pre>518</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Memory Usage</pre></td></tr><tr><td class='line-number'><a name='L519' href='#L519'><pre>519</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - phi-2 (32 layers): ~6.4 GB</pre></td></tr><tr><td class='line-number'><a name='L520' href='#L520'><pre>520</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - Per layer: 2  hidden_dim  intermediate_dim  4 bytes</pre></td></tr><tr><td class='line-number'><a name='L521' href='#L521'><pre>521</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L522' href='#L522'><pre>522</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L523' href='#L523'><pre>523</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - Total memory allocated in bytes</pre></td></tr><tr><td class='line-number'><a name='L524' href='#L524'><pre>524</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - Number of layers cached</pre></td></tr><tr><td class='line-number'><a name='L525' href='#L525'><pre>525</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L526' href='#L526'><pre>526</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L527' href='#L527'><pre>527</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if dequantization fails</pre></td></tr><tr><td class='line-number'><a name='L528' href='#L528'><pre>528</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn warmup_gpu_cache(&amp;self) -&gt; Result&lt;(usize, usize)&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L529' href='#L529'><pre>529</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>config</span> = <span class='region red'>&amp;self.model.config</span>;</pre></td></tr><tr><td class='line-number'><a name='L530' href='#L530'><pre>530</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L531' href='#L531'><pre>531</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>intermediate_dim</span> = <span class='region red'>config.intermediate_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L532' href='#L532'><pre>532</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>num_layers</span> = <span class='region red'>self.model.layers</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L533' href='#L533'><pre>533</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L534' href='#L534'><pre>534</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Create cache with model dimensions</pre></td></tr><tr><td class='line-number'><a name='L535' href='#L535'><pre>535</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>cache</span> = <span class='region red'>DequantizedWeightCache::new</span>(<span class='region red'>hidden_dim</span>, <span class='region red'>intermediate_dim</span>, <span class='region red'>num_layers</span>);</pre></td></tr><tr><td class='line-number'><a name='L536' href='#L536'><pre>536</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L537' href='#L537'><pre>537</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Dequantize each layer&apos;s FFN weights</pre></td></tr><tr><td class='line-number'><a name='L538' href='#L538'><pre>538</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Note: warmup closure can&apos;t return Result, so we use unwrap_or_default</pre></td></tr><tr><td class='line-number'><a name='L539' href='#L539'><pre>539</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // for robustness. In production, use warmup_gpu_cache_checked() for error handling.</pre></td></tr><tr><td class='line-number'><a name='L540' href='#L540'><pre>540</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>cache</span>.<span class='region red'>warmup</span>(|layer_idx| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L541' href='#L541'><pre>541</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>layer</span> = <span class='region red'>&amp;self.model.layers[layer_idx]</span>;</pre></td></tr><tr><td class='line-number'><a name='L542' href='#L542'><pre>542</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L543' href='#L543'><pre>543</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // Dequantize using model&apos;s dequantize_weight method</pre></td></tr><tr><td class='line-number'><a name='L544' href='#L544'><pre>544</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>up</span> = <span class='region red'>self</span></pre></td></tr><tr><td class='line-number'><a name='L545' href='#L545'><pre>545</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                .model</span></pre></td></tr><tr><td class='line-number'><a name='L546' href='#L546'><pre>546</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>dequantize_weight</span>(<span class='region red'>&amp;layer.ffn_up_weight</span>)</pre></td></tr><tr><td class='line-number'><a name='L547' href='#L547'><pre>547</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>unwrap_or_default</span>();</pre></td></tr><tr><td class='line-number'><a name='L548' href='#L548'><pre>548</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>down</span> = <span class='region red'>self</span></pre></td></tr><tr><td class='line-number'><a name='L549' href='#L549'><pre>549</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                .model</span></pre></td></tr><tr><td class='line-number'><a name='L550' href='#L550'><pre>550</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>dequantize_weight</span>(<span class='region red'>&amp;layer.ffn_down_weight</span>)</pre></td></tr><tr><td class='line-number'><a name='L551' href='#L551'><pre>551</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>unwrap_or_default</span>();</pre></td></tr><tr><td class='line-number'><a name='L552' href='#L552'><pre>552</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L553' href='#L553'><pre>553</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>(up, down)</span></pre></td></tr><tr><td class='line-number'><a name='L554' href='#L554'><pre>554</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span>);</pre></td></tr><tr><td class='line-number'><a name='L555' href='#L555'><pre>555</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L556' href='#L556'><pre>556</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>memory_bytes</span> = <span class='region red'>cache</span>.<span class='region red'>memory_bytes</span>();</pre></td></tr><tr><td class='line-number'><a name='L557' href='#L557'><pre>557</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>cached_count</span> = <span class='region red'>cache</span>.<span class='region red'>cached_count</span>();</pre></td></tr><tr><td class='line-number'><a name='L558' href='#L558'><pre>558</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L559' href='#L559'><pre>559</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Store in the cache field</pre></td></tr><tr><td class='line-number'><a name='L560' href='#L560'><pre>560</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut cache_guard</span> =</pre></td></tr><tr><td class='line-number'><a name='L561' href='#L561'><pre>561</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>self.dequant_cache</span></pre></td></tr><tr><td class='line-number'><a name='L562' href='#L562'><pre>562</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>write</span>()</pre></td></tr><tr><td class='line-number'><a name='L563' href='#L563'><pre>563</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>map_err</span>(|_| RealizarError::UnsupportedOperation {</pre></td></tr><tr><td class='line-number'><a name='L564' href='#L564'><pre>564</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    operation: <span class='region red'>&quot;warmup_gpu_cache&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L565' href='#L565'><pre>565</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    reason: <span class='region red'>&quot;Cache lock poisoned&quot;</span>.<span class='region red'>to_string</span>(),</pre></td></tr><tr><td class='line-number'><a name='L566' href='#L566'><pre>566</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L567' href='#L567'><pre>567</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>*cache_guard</span> = <span class='region red'>Some(cache)</span>;</pre></td></tr><tr><td class='line-number'><a name='L568' href='#L568'><pre>568</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L569' href='#L569'><pre>569</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok((memory_bytes, cached_count))</span></pre></td></tr><tr><td class='line-number'><a name='L570' href='#L570'><pre>570</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L571' href='#L571'><pre>571</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L572' href='#L572'><pre>572</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Check if GPU cache is warmed up</pre></td></tr><tr><td class='line-number'><a name='L573' href='#L573'><pre>573</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn is_gpu_cache_warm(&amp;self) -&gt; bool</span> {</pre></td></tr><tr><td class='line-number'><a name='L574' href='#L574'><pre>574</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>self.dequant_cache</span></pre></td></tr><tr><td class='line-number'><a name='L575' href='#L575'><pre>575</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>read</span>()</pre></td></tr><tr><td class='line-number'><a name='L576' href='#L576'><pre>576</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>map</span>(|guard| <span class='region red'>guard</span>.<span class='region red'>is_some</span>())</pre></td></tr><tr><td class='line-number'><a name='L577' href='#L577'><pre>577</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>unwrap_or</span>(false)</pre></td></tr><tr><td class='line-number'><a name='L578' href='#L578'><pre>578</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L579' href='#L579'><pre>579</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L580' href='#L580'><pre>580</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Get GPU cache memory usage in bytes</pre></td></tr><tr><td class='line-number'><a name='L581' href='#L581'><pre>581</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn gpu_cache_memory(&amp;self) -&gt; usize</span> {</pre></td></tr><tr><td class='line-number'><a name='L582' href='#L582'><pre>582</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>self.dequant_cache</span></pre></td></tr><tr><td class='line-number'><a name='L583' href='#L583'><pre>583</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>read</span>()</pre></td></tr><tr><td class='line-number'><a name='L584' href='#L584'><pre>584</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>ok</span>()</pre></td></tr><tr><td class='line-number'><a name='L585' href='#L585'><pre>585</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>and_then</span>(|guard| <span class='region red'>guard.as_ref()</span>.<span class='region red'>map</span>(DequantizedWeightCache::memory_bytes))</pre></td></tr><tr><td class='line-number'><a name='L586' href='#L586'><pre>586</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>unwrap_or</span>(0)</pre></td></tr><tr><td class='line-number'><a name='L587' href='#L587'><pre>587</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L588' href='#L588'><pre>588</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L589' href='#L589'><pre>589</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Get dequantized weights for a layer (for GPU batch FFN)</pre></td></tr><tr><td class='line-number'><a name='L590' href='#L590'><pre>590</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L591' href='#L591'><pre>591</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns None if cache not warmed up or layer not found.</pre></td></tr><tr><td class='line-number'><a name='L592' href='#L592'><pre>592</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn get_dequantized_ffn_weights(&amp;self, layer_idx: usize) -&gt; Option&lt;DequantizedFFNWeights&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L593' href='#L593'><pre>593</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>self.dequant_cache</span></pre></td></tr><tr><td class='line-number'><a name='L594' href='#L594'><pre>594</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>read</span>()</pre></td></tr><tr><td class='line-number'><a name='L595' href='#L595'><pre>595</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>ok</span>()</pre></td></tr><tr><td class='line-number'><a name='L596' href='#L596'><pre>596</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>and_then</span>(|guard| <span class='region red'>guard.as_ref()</span>.<span class='region red'>and_then</span>(|c| <span class='region red'>c</span>.<span class='region red'>get</span>(<span class='region red'>layer_idx</span>)))</pre></td></tr><tr><td class='line-number'><a name='L597' href='#L597'><pre>597</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L598' href='#L598'><pre>598</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L599' href='#L599'><pre>599</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Batch FFN forward pass using GPU (PARITY-019)</pre></td></tr><tr><td class='line-number'><a name='L600' href='#L600'><pre>600</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L601' href='#L601'><pre>601</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Processes multiple tokens in parallel using GPU GEMM.</pre></td></tr><tr><td class='line-number'><a name='L602' href='#L602'><pre>602</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Requires cache to be warmed up via `warmup_gpu_cache()`.</pre></td></tr><tr><td class='line-number'><a name='L603' href='#L603'><pre>603</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L604' href='#L604'><pre>604</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L605' href='#L605'><pre>605</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `hidden_states` - Input tensor [batch_size  hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L606' href='#L606'><pre>606</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `layer_idx` - Layer index for weight lookup</pre></td></tr><tr><td class='line-number'><a name='L607' href='#L607'><pre>607</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L608' href='#L608'><pre>608</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L609' href='#L609'><pre>609</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Output tensor [batch_size  hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L610' href='#L610'><pre>610</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L611' href='#L611'><pre>611</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L612' href='#L612'><pre>612</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if cache not warmed or GPU operations fail</pre></td></tr><tr><td class='line-number'><a name='L613' href='#L613'><pre>613</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Batch FFN using CUDA when available</pre></td></tr><tr><td class='line-number'><a name='L614' href='#L614'><pre>614</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L615' href='#L615'><pre>615</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Uses CudaScheduler first (no buffer limits), falls back to HybridScheduler (wgpu).</pre></td></tr><tr><td class='line-number'><a name='L616' href='#L616'><pre>616</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// This bypasses the wgpu 256MB buffer limit that was blocking GPU batch inference.</pre></td></tr><tr><td class='line-number'><a name='L617' href='#L617'><pre>617</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn batch_ffn_gpu(&amp;self, hidden_states: &amp;[f32], layer_idx: usize) -&gt; Result&lt;Vec&lt;f32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L618' href='#L618'><pre>618</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>config</span> = <span class='region red'>&amp;self.model.config</span>;</pre></td></tr><tr><td class='line-number'><a name='L619' href='#L619'><pre>619</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L620' href='#L620'><pre>620</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>intermediate_dim</span> = <span class='region red'>config.intermediate_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L621' href='#L621'><pre>621</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>batch_size</span> = <span class='region red'>hidden_states</span><span class='region red'>.len() / hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L622' href='#L622'><pre>622</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L623' href='#L623'><pre>623</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>batch_size == 0</span> {</pre></td></tr><tr><td class='line-number'><a name='L624' href='#L624'><pre>624</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Err(RealizarError::UnsupportedOperation {</span></pre></td></tr><tr><td class='line-number'><a name='L625' href='#L625'><pre>625</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                operation: </span><span class='region red'>&quot;batch_ffn_gpu&quot;</span><span class='region red'>.</span><span class='region red'>to_string</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L626' href='#L626'><pre>626</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                reason: &quot;Empty batch&quot;.to_string(),</span></pre></td></tr><tr><td class='line-number'><a name='L627' href='#L627'><pre>627</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            })</span>;</pre></td></tr><tr><td class='line-number'><a name='L628' href='#L628'><pre>628</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L629' href='#L629'><pre>629</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L630' href='#L630'><pre>630</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Get cached weights</pre></td></tr><tr><td class='line-number'><a name='L631' href='#L631'><pre>631</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>weights</span> = <span class='region red'>self</span>.<span class='region red'>get_dequantized_ffn_weights</span>(<span class='region red'>layer_idx</span>).<span class='region red'>ok_or_else</span>(|| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L632' href='#L632'><pre>632</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>RealizarError::UnsupportedOperation {</span></pre></td></tr><tr><td class='line-number'><a name='L633' href='#L633'><pre>633</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                operation: </span><span class='region red'>&quot;batch_ffn_gpu&quot;</span><span class='region red'>.</span><span class='region red'>to_string</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L634' href='#L634'><pre>634</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                reason: format!(</span></pre></td></tr><tr><td class='line-number'><a name='L635' href='#L635'><pre>635</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    &quot;Layer {} not cached. Call warmup_gpu_cache() first.&quot;,</span></pre></td></tr><tr><td class='line-number'><a name='L636' href='#L636'><pre>636</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    layer_idx</span></pre></td></tr><tr><td class='line-number'><a name='L637' href='#L637'><pre>637</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                ),</span></pre></td></tr><tr><td class='line-number'><a name='L638' href='#L638'><pre>638</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            }</span></pre></td></tr><tr><td class='line-number'><a name='L639' href='#L639'><pre>639</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L640' href='#L640'><pre>640</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L641' href='#L641'><pre>641</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-103: Up projection preferring CUDA</pre></td></tr><tr><td class='line-number'><a name='L642' href='#L642'><pre>642</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut intermediate</span> = <span class='region red'>self</span>.<span class='region red'>batch_matmul_gpu_prefer_cuda</span>(</pre></td></tr><tr><td class='line-number'><a name='L643' href='#L643'><pre>643</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_states</span>,</pre></td></tr><tr><td class='line-number'><a name='L644' href='#L644'><pre>644</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>&amp;weights.up</span>,</pre></td></tr><tr><td class='line-number'><a name='L645' href='#L645'><pre>645</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>batch_size</span>,</pre></td></tr><tr><td class='line-number'><a name='L646' href='#L646'><pre>646</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L647' href='#L647'><pre>647</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>intermediate_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L648' href='#L648'><pre>648</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L649' href='#L649'><pre>649</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L650' href='#L650'><pre>650</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Add up bias if present</pre></td></tr><tr><td class='line-number'><a name='L651' href='#L651'><pre>651</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>weights.up_bias</span> {</pre></td></tr><tr><td class='line-number'><a name='L652' href='#L652'><pre>652</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            for <span class='region red'>b</span> in 0..<span class='region red'>batch_size</span> {</pre></td></tr><tr><td class='line-number'><a name='L653' href='#L653'><pre>653</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for <span class='region red'>i</span> in 0..<span class='region red'>intermediate_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L654' href='#L654'><pre>654</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>intermediate</span><span class='region red'>[b * intermediate_dim + i]</span><span class='region red'> += bias[i];</span></pre></td></tr><tr><td class='line-number'><a name='L655' href='#L655'><pre>655</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L656' href='#L656'><pre>656</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L657' href='#L657'><pre>657</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L658' href='#L658'><pre>658</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L659' href='#L659'><pre>659</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // GELU activation (CPU - fused in future)</pre></td></tr><tr><td class='line-number'><a name='L660' href='#L660'><pre>660</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        for <span class='region red'>x</span> in <span class='region red'>&amp;mut intermediate</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L661' href='#L661'><pre>661</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            let </span><span class='region red'>x64</span><span class='region red'> = </span><span class='region red'>*x as f64</span><span class='region red'>;</span></pre></td></tr><tr><td class='line-number'><a name='L662' href='#L662'><pre>662</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            *x = (</span><span class='region red'>x64</span></pre></td></tr><tr><td class='line-number'><a name='L663' href='#L663'><pre>663</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                * 0.5</span><span class='red'></span></pre></td></tr><tr><td class='line-number'><a name='L664' href='#L664'><pre>664</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                * (1.0 + (</span><span class='region red'>x64 * 0.797_884_560_8</span><span class='region red'> * (1.0 + 0.044_715 * x64 * x64)).tanh()))</span></pre></td></tr><tr><td class='line-number'><a name='L665' href='#L665'><pre>665</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                as f32;</span></pre></td></tr><tr><td class='line-number'><a name='L666' href='#L666'><pre>666</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        }</span></pre></td></tr><tr><td class='line-number'><a name='L667' href='#L667'><pre>667</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L668' href='#L668'><pre>668</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-103: Down projection preferring CUDA</pre></td></tr><tr><td class='line-number'><a name='L669' href='#L669'><pre>669</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut output</span> = <span class='region red'>self</span>.<span class='region red'>batch_matmul_gpu_prefer_cuda</span>(</pre></td></tr><tr><td class='line-number'><a name='L670' href='#L670'><pre>670</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>&amp;intermediate</span>,</pre></td></tr><tr><td class='line-number'><a name='L671' href='#L671'><pre>671</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>&amp;weights.down</span>,</pre></td></tr><tr><td class='line-number'><a name='L672' href='#L672'><pre>672</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>batch_size</span>,</pre></td></tr><tr><td class='line-number'><a name='L673' href='#L673'><pre>673</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>intermediate_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L674' href='#L674'><pre>674</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L675' href='#L675'><pre>675</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L676' href='#L676'><pre>676</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L677' href='#L677'><pre>677</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Add down bias if present</pre></td></tr><tr><td class='line-number'><a name='L678' href='#L678'><pre>678</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>weights.down_bias</span> {</pre></td></tr><tr><td class='line-number'><a name='L679' href='#L679'><pre>679</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            for <span class='region red'>b</span> in 0..<span class='region red'>batch_size</span> {</pre></td></tr><tr><td class='line-number'><a name='L680' href='#L680'><pre>680</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for <span class='region red'>i</span> in 0..<span class='region red'>hidden_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L681' href='#L681'><pre>681</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>output</span><span class='region red'>[b * hidden_dim + i]</span><span class='region red'> += bias[i];</span></pre></td></tr><tr><td class='line-number'><a name='L682' href='#L682'><pre>682</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L683' href='#L683'><pre>683</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L684' href='#L684'><pre>684</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L685' href='#L685'><pre>685</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L686' href='#L686'><pre>686</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(output)</span></pre></td></tr><tr><td class='line-number'><a name='L687' href='#L687'><pre>687</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L688' href='#L688'><pre>688</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L689' href='#L689'><pre>689</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// PARITY-103: Batch QKV projection using CUDA when available</pre></td></tr><tr><td class='line-number'><a name='L690' href='#L690'><pre>690</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L691' href='#L691'><pre>691</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Projects hidden states to Q, K, V for all requests in batch.</pre></td></tr><tr><td class='line-number'><a name='L692' href='#L692'><pre>692</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// [batch, hidden] @ [hidden, 3*hidden] = [batch, 3*hidden]</pre></td></tr><tr><td class='line-number'><a name='L693' href='#L693'><pre>693</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L694' href='#L694'><pre>694</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Uses CudaScheduler first (no buffer limits), falls back to HybridScheduler (wgpu).</pre></td></tr><tr><td class='line-number'><a name='L695' href='#L695'><pre>695</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L696' href='#L696'><pre>696</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L697' href='#L697'><pre>697</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `hidden_states` - Flattened hidden states [batch * hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L698' href='#L698'><pre>698</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `layer_idx` - Layer index for weight lookup</pre></td></tr><tr><td class='line-number'><a name='L699' href='#L699'><pre>699</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L700' href='#L700'><pre>700</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L701' href='#L701'><pre>701</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Flattened QKV projections [batch * 3 * hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L702' href='#L702'><pre>702</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L703' href='#L703'><pre>703</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn batch_qkv_projection_gpu(</span></pre></td></tr><tr><td class='line-number'><a name='L704' href='#L704'><pre>704</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L705' href='#L705'><pre>705</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        hidden_states: &amp;[f32],</span></pre></td></tr><tr><td class='line-number'><a name='L706' href='#L706'><pre>706</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        layer_idx: usize,</span></pre></td></tr><tr><td class='line-number'><a name='L707' href='#L707'><pre>707</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L708' href='#L708'><pre>708</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>self.model.config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L709' href='#L709'><pre>709</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>batch_size</span> = <span class='region red'>hidden_states</span><span class='region red'>.len() / hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L710' href='#L710'><pre>710</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>qkv_dim</span> = <span class='region red'>3 * hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L711' href='#L711'><pre>711</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L712' href='#L712'><pre>712</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>batch_size == 0</span> {</pre></td></tr><tr><td class='line-number'><a name='L713' href='#L713'><pre>713</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Ok(Vec::new())</span>;</pre></td></tr><tr><td class='line-number'><a name='L714' href='#L714'><pre>714</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L715' href='#L715'><pre>715</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L716' href='#L716'><pre>716</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>layer</span> = <span class='region red'>&amp;self.model.layers[layer_idx]</span>;</pre></td></tr><tr><td class='line-number'><a name='L717' href='#L717'><pre>717</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L718' href='#L718'><pre>718</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Dequantize QKV weight for GPU GEMM</pre></td></tr><tr><td class='line-number'><a name='L719' href='#L719'><pre>719</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>qkv_weight</span> = <span class='region red'>self.model</span>.<span class='region red'>dequantize_qkv</span>(<span class='region red'>&amp;layer.qkv_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L720' href='#L720'><pre>720</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L721' href='#L721'><pre>721</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-103: QKV projection preferring CUDA</pre></td></tr><tr><td class='line-number'><a name='L722' href='#L722'><pre>722</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut qkv</span> = <span class='region red'>self</span>.<span class='region red'>batch_matmul_gpu_prefer_cuda</span>(</pre></td></tr><tr><td class='line-number'><a name='L723' href='#L723'><pre>723</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_states</span>,</pre></td></tr><tr><td class='line-number'><a name='L724' href='#L724'><pre>724</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>&amp;qkv_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L725' href='#L725'><pre>725</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>batch_size</span>,</pre></td></tr><tr><td class='line-number'><a name='L726' href='#L726'><pre>726</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L727' href='#L727'><pre>727</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>qkv_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L728' href='#L728'><pre>728</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L729' href='#L729'><pre>729</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L730' href='#L730'><pre>730</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Add bias if present</pre></td></tr><tr><td class='line-number'><a name='L731' href='#L731'><pre>731</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>layer.qkv_bias</span> {</pre></td></tr><tr><td class='line-number'><a name='L732' href='#L732'><pre>732</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            for <span class='region red'>b</span> in 0..<span class='region red'>batch_size</span> {</pre></td></tr><tr><td class='line-number'><a name='L733' href='#L733'><pre>733</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for <span class='region red'>i</span> in 0..<span class='region red'>qkv_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L734' href='#L734'><pre>734</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>qkv</span><span class='region red'>[b * qkv_dim + i]</span><span class='region red'> += bias[i];</span></pre></td></tr><tr><td class='line-number'><a name='L735' href='#L735'><pre>735</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L736' href='#L736'><pre>736</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L737' href='#L737'><pre>737</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L738' href='#L738'><pre>738</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L739' href='#L739'><pre>739</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(qkv)</span></pre></td></tr><tr><td class='line-number'><a name='L740' href='#L740'><pre>740</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L741' href='#L741'><pre>741</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L742' href='#L742'><pre>742</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Batch attention output projection using GPU GEMM (PARITY-024)</pre></td></tr><tr><td class='line-number'><a name='L743' href='#L743'><pre>743</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L744' href='#L744'><pre>744</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Projects attention outputs for all requests in batch.</pre></td></tr><tr><td class='line-number'><a name='L745' href='#L745'><pre>745</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// [batch, hidden] @ [hidden, hidden] = [batch, hidden]</pre></td></tr><tr><td class='line-number'><a name='L746' href='#L746'><pre>746</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L747' href='#L747'><pre>747</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L748' href='#L748'><pre>748</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `attention_outputs` - Flattened attention outputs [batch * hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L749' href='#L749'><pre>749</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `layer_idx` - Layer index for weight lookup</pre></td></tr><tr><td class='line-number'><a name='L750' href='#L750'><pre>750</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L751' href='#L751'><pre>751</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L752' href='#L752'><pre>752</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Flattened projected outputs [batch * hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L753' href='#L753'><pre>753</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L754' href='#L754'><pre>754</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn batch_attention_output_gpu(</span></pre></td></tr><tr><td class='line-number'><a name='L755' href='#L755'><pre>755</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L756' href='#L756'><pre>756</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        attention_outputs: &amp;[f32],</span></pre></td></tr><tr><td class='line-number'><a name='L757' href='#L757'><pre>757</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        layer_idx: usize,</span></pre></td></tr><tr><td class='line-number'><a name='L758' href='#L758'><pre>758</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L759' href='#L759'><pre>759</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>self.model.config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L760' href='#L760'><pre>760</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>batch_size</span> = <span class='region red'>attention_outputs</span><span class='region red'>.len() / hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L761' href='#L761'><pre>761</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L762' href='#L762'><pre>762</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>batch_size == 0</span> {</pre></td></tr><tr><td class='line-number'><a name='L763' href='#L763'><pre>763</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Ok(Vec::new())</span>;</pre></td></tr><tr><td class='line-number'><a name='L764' href='#L764'><pre>764</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L765' href='#L765'><pre>765</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L766' href='#L766'><pre>766</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>layer</span> = <span class='region red'>&amp;self.model.layers[layer_idx]</span>;</pre></td></tr><tr><td class='line-number'><a name='L767' href='#L767'><pre>767</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L768' href='#L768'><pre>768</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Dequantize output weight for GPU GEMM</pre></td></tr><tr><td class='line-number'><a name='L769' href='#L769'><pre>769</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>output_weight</span> = <span class='region red'>self.model</span>.<span class='region red'>dequantize_weight</span>(<span class='region red'>&amp;layer.attn_output_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L770' href='#L770'><pre>770</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L771' href='#L771'><pre>771</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-103: Output projection preferring CUDA (bypasses wgpu 256MB limit)</pre></td></tr><tr><td class='line-number'><a name='L772' href='#L772'><pre>772</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // [batch, hidden] @ [hidden, hidden] = [batch, hidden]</pre></td></tr><tr><td class='line-number'><a name='L773' href='#L773'><pre>773</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut output</span> = <span class='region red'>self</span>.<span class='region red'>batch_matmul_gpu_prefer_cuda</span>(</pre></td></tr><tr><td class='line-number'><a name='L774' href='#L774'><pre>774</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>attention_outputs</span>,</pre></td></tr><tr><td class='line-number'><a name='L775' href='#L775'><pre>775</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>&amp;output_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L776' href='#L776'><pre>776</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>batch_size</span>,</pre></td></tr><tr><td class='line-number'><a name='L777' href='#L777'><pre>777</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L778' href='#L778'><pre>778</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L779' href='#L779'><pre>779</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L780' href='#L780'><pre>780</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L781' href='#L781'><pre>781</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Add bias if present</pre></td></tr><tr><td class='line-number'><a name='L782' href='#L782'><pre>782</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>layer.attn_output_bias</span> {</pre></td></tr><tr><td class='line-number'><a name='L783' href='#L783'><pre>783</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            for <span class='region red'>b</span> in 0..<span class='region red'>batch_size</span> {</pre></td></tr><tr><td class='line-number'><a name='L784' href='#L784'><pre>784</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for <span class='region red'>i</span> in 0..<span class='region red'>hidden_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L785' href='#L785'><pre>785</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>output</span><span class='region red'>[b * hidden_dim + i]</span><span class='region red'> += bias[i];</span></pre></td></tr><tr><td class='line-number'><a name='L786' href='#L786'><pre>786</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L787' href='#L787'><pre>787</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L788' href='#L788'><pre>788</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L789' href='#L789'><pre>789</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L790' href='#L790'><pre>790</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(output)</span></pre></td></tr><tr><td class='line-number'><a name='L791' href='#L791'><pre>791</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L792' href='#L792'><pre>792</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L793' href='#L793'><pre>793</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Batch LM head projection using GPU GEMM (PARITY-025)</pre></td></tr><tr><td class='line-number'><a name='L794' href='#L794'><pre>794</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L795' href='#L795'><pre>795</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Projects hidden states to vocabulary logits for all requests in batch.</pre></td></tr><tr><td class='line-number'><a name='L796' href='#L796'><pre>796</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// [batch, hidden] @ [hidden, vocab] = [batch, vocab]</pre></td></tr><tr><td class='line-number'><a name='L797' href='#L797'><pre>797</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L798' href='#L798'><pre>798</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L799' href='#L799'><pre>799</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `hidden_states` - Flattened normalized hidden states [batch * hidden_dim]</pre></td></tr><tr><td class='line-number'><a name='L800' href='#L800'><pre>800</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L801' href='#L801'><pre>801</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L802' href='#L802'><pre>802</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Flattened logits [batch * vocab_size]</pre></td></tr><tr><td class='line-number'><a name='L803' href='#L803'><pre>803</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    #[cfg(feature = &quot;gpu&quot;)]</pre></td></tr><tr><td class='line-number'><a name='L804' href='#L804'><pre>804</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn batch_lm_head_gpu(&amp;self, hidden_states: &amp;[f32]) -&gt; Result&lt;Vec&lt;f32&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L805' href='#L805'><pre>805</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>self.model.config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L806' href='#L806'><pre>806</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>vocab_size</span> = <span class='region red'>self.model.config.vocab_size</span>;</pre></td></tr><tr><td class='line-number'><a name='L807' href='#L807'><pre>807</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>batch_size</span> = <span class='region red'>hidden_states</span><span class='region red'>.len() / hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L808' href='#L808'><pre>808</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L809' href='#L809'><pre>809</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>batch_size == 0</span> {</pre></td></tr><tr><td class='line-number'><a name='L810' href='#L810'><pre>810</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Ok(Vec::new())</span>;</pre></td></tr><tr><td class='line-number'><a name='L811' href='#L811'><pre>811</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L812' href='#L812'><pre>812</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L813' href='#L813'><pre>813</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Dequantize LM head weight for GPU GEMM</pre></td></tr><tr><td class='line-number'><a name='L814' href='#L814'><pre>814</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>lm_head_weight</span> = <span class='region red'>self.model</span>.<span class='region red'>dequantize_weight</span>(<span class='region red'>&amp;self.model.lm_head_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L815' href='#L815'><pre>815</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L816' href='#L816'><pre>816</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-103: LM head projection preferring CUDA (bypasses wgpu 256MB limit)</pre></td></tr><tr><td class='line-number'><a name='L817' href='#L817'><pre>817</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // [batch, hidden] @ [hidden, vocab] = [batch, vocab]</pre></td></tr><tr><td class='line-number'><a name='L818' href='#L818'><pre>818</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut logits</span> = <span class='region red'>self</span>.<span class='region red'>batch_matmul_gpu_prefer_cuda</span>(</pre></td></tr><tr><td class='line-number'><a name='L819' href='#L819'><pre>819</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_states</span>,</pre></td></tr><tr><td class='line-number'><a name='L820' href='#L820'><pre>820</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>&amp;lm_head_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L821' href='#L821'><pre>821</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>batch_size</span>,</pre></td></tr><tr><td class='line-number'><a name='L822' href='#L822'><pre>822</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>hidden_dim</span>,</pre></td></tr><tr><td class='line-number'><a name='L823' href='#L823'><pre>823</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>vocab_size</span>,</pre></td></tr><tr><td class='line-number'><a name='L824' href='#L824'><pre>824</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L825' href='#L825'><pre>825</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L826' href='#L826'><pre>826</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Add bias if present</pre></td></tr><tr><td class='line-number'><a name='L827' href='#L827'><pre>827</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>self.model.lm_head_bias</span> {</pre></td></tr><tr><td class='line-number'><a name='L828' href='#L828'><pre>828</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            for <span class='region red'>b</span> in 0..<span class='region red'>batch_size</span> {</pre></td></tr><tr><td class='line-number'><a name='L829' href='#L829'><pre>829</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for <span class='region red'>i</span> in 0..<span class='region red'>vocab_size</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L830' href='#L830'><pre>830</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>logits</span><span class='region red'>[b * vocab_size + i]</span><span class='region red'> += bias[i];</span></pre></td></tr><tr><td class='line-number'><a name='L831' href='#L831'><pre>831</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L832' href='#L832'><pre>832</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L833' href='#L833'><pre>833</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L834' href='#L834'><pre>834</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L835' href='#L835'><pre>835</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(logits)</span></pre></td></tr><tr><td class='line-number'><a name='L836' href='#L836'><pre>836</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L837' href='#L837'><pre>837</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L838' href='#L838'><pre>838</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Batch generation with GPU-accelerated FFN (PARITY-020)</pre></td></tr><tr><td class='line-number'><a name='L839' href='#L839'><pre>839</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L840' href='#L840'><pre>840</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Processes multiple prompts in parallel using GPU batch operations.</pre></td></tr><tr><td class='line-number'><a name='L841' href='#L841'><pre>841</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// The key optimization is converting MATVEC (single token) to GEMM (batch tokens).</pre></td></tr><tr><td class='line-number'><a name='L842' href='#L842'><pre>842</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L843' href='#L843'><pre>843</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Architecture</pre></td></tr><tr><td class='line-number'><a name='L844' href='#L844'><pre>844</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - Attention: CPU with KV cache (MATVEC is faster on CPU)</pre></td></tr><tr><td class='line-number'><a name='L845' href='#L845'><pre>845</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - FFN: GPU with batch GEMM (batch_size  32 uses GPU)</pre></td></tr><tr><td class='line-number'><a name='L846' href='#L846'><pre>846</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - Sampling: CPU (negligible compared to matmul)</pre></td></tr><tr><td class='line-number'><a name='L847' href='#L847'><pre>847</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L848' href='#L848'><pre>848</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L849' href='#L849'><pre>849</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `prompts` - Multiple prompts to process in parallel [num_prompts][seq_len]</pre></td></tr><tr><td class='line-number'><a name='L850' href='#L850'><pre>850</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `config` - Generation configuration (shared across all prompts)</pre></td></tr><tr><td class='line-number'><a name='L851' href='#L851'><pre>851</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L852' href='#L852'><pre>852</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L853' href='#L853'><pre>853</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Generated sequences for each prompt [num_prompts][generated_len]</pre></td></tr><tr><td class='line-number'><a name='L854' href='#L854'><pre>854</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L855' href='#L855'><pre>855</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Errors</pre></td></tr><tr><td class='line-number'><a name='L856' href='#L856'><pre>856</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns error if GPU cache not warmed up or generation fails</pre></td></tr><tr><td class='line-number'><a name='L857' href='#L857'><pre>857</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L858' href='#L858'><pre>858</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Performance</pre></td></tr><tr><td class='line-number'><a name='L859' href='#L859'><pre>859</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - Single prompt: ~5 tok/s (CPU-bound, no batching benefit)</pre></td></tr><tr><td class='line-number'><a name='L860' href='#L860'><pre>860</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - 32 prompts: ~150 tok/s total (~4.7 tok/s per prompt)</pre></td></tr><tr><td class='line-number'><a name='L861' href='#L861'><pre>861</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - 64 prompts: ~280 tok/s total (~4.4 tok/s per prompt, memory-bound)</pre></td></tr><tr><td class='line-number'><a name='L862' href='#L862'><pre>862</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn batch_generate_gpu(</span></pre></td></tr><tr><td class='line-number'><a name='L863' href='#L863'><pre>863</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L864' href='#L864'><pre>864</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        prompts: &amp;[Vec&lt;u32&gt;],</span></pre></td></tr><tr><td class='line-number'><a name='L865' href='#L865'><pre>865</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        config: &amp;QuantizedGenerateConfig,</span></pre></td></tr><tr><td class='line-number'><a name='L866' href='#L866'><pre>866</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;Vec&lt;Vec&lt;u32&gt;&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L867' href='#L867'><pre>867</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>prompts</span>.<span class='region red'>is_empty</span>() {</pre></td></tr><tr><td class='line-number'><a name='L868' href='#L868'><pre>868</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Ok(Vec::new())</span>;</pre></td></tr><tr><td class='line-number'><a name='L869' href='#L869'><pre>869</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L870' href='#L870'><pre>870</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L871' href='#L871'><pre>871</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Verify GPU cache is warmed up</pre></td></tr><tr><td class='line-number'><a name='L872' href='#L872'><pre>872</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>!self.is_gpu_cache_warm()</span> {</pre></td></tr><tr><td class='line-number'><a name='L873' href='#L873'><pre>873</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Err(RealizarError::UnsupportedOperation {</span></pre></td></tr><tr><td class='line-number'><a name='L874' href='#L874'><pre>874</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                operation: </span><span class='region red'>&quot;batch_generate_gpu&quot;</span><span class='region red'>.</span><span class='region red'>to_string</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L875' href='#L875'><pre>875</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                reason: &quot;GPU cache not warmed up. Call warmup_gpu_cache() first.&quot;.to_string(),</span></pre></td></tr><tr><td class='line-number'><a name='L876' href='#L876'><pre>876</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            })</span>;</pre></td></tr><tr><td class='line-number'><a name='L877' href='#L877'><pre>877</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L878' href='#L878'><pre>878</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L879' href='#L879'><pre>879</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>num_prompts</span> = <span class='region red'>prompts</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L880' href='#L880'><pre>880</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>max_seq_len</span> = <span class='region red'>prompts</span><span class='region red'>.</span><span class='region red'>iter</span><span class='region red'>().</span><span class='region red'>map</span><span class='region red'>(Vec::len).</span><span class='region red'>max</span><span class='region red'>().unwrap_or(0) + config.max_tokens</span>;</pre></td></tr><tr><td class='line-number'><a name='L881' href='#L881'><pre>881</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L882' href='#L882'><pre>882</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Initialize KV caches for each prompt</pre></td></tr><tr><td class='line-number'><a name='L883' href='#L883'><pre>883</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut caches</span>: <span class='region red'>Vec&lt;OwnedQuantizedKVCache&gt;</span> = <span class='region red'>prompts</span></pre></td></tr><tr><td class='line-number'><a name='L884' href='#L884'><pre>884</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>iter</span>()</pre></td></tr><tr><td class='line-number'><a name='L885' href='#L885'><pre>885</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>map</span>(|_| <span class='region red'>OwnedQuantizedKVCache::from_config</span>(<span class='region red'>&amp;self.model.config</span>, <span class='region red'>max_seq_len</span>))</pre></td></tr><tr><td class='line-number'><a name='L886' href='#L886'><pre>886</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L887' href='#L887'><pre>887</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L888' href='#L888'><pre>888</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Initialize token sequences (copy prompts)</pre></td></tr><tr><td class='line-number'><a name='L889' href='#L889'><pre>889</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut sequences</span>: <span class='region red'>Vec&lt;Vec&lt;u32&gt;&gt;</span> = <span class='region red'>prompts</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L890' href='#L890'><pre>890</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L891' href='#L891'><pre>891</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Track generation progress per prompt</pre></td></tr><tr><td class='line-number'><a name='L892' href='#L892'><pre>892</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut done</span>: <span class='region red'>Vec&lt;bool&gt;</span> = <span class='region red'>vec!</span>[false; <span class='region red'>num_prompts</span>];</pre></td></tr><tr><td class='line-number'><a name='L893' href='#L893'><pre>893</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L894' href='#L894'><pre>894</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-097: Parallel prefill across prompts using rayon</pre></td></tr><tr><td class='line-number'><a name='L895' href='#L895'><pre>895</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Each prompt&apos;s prefill is independent (different KV cache)</pre></td></tr><tr><td class='line-number'><a name='L896' href='#L896'><pre>896</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Model is shared immutably (&amp;self), caches are mutated independently</pre></td></tr><tr><td class='line-number'><a name='L897' href='#L897'><pre>897</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        use rayon::prelude::*;</pre></td></tr><tr><td class='line-number'><a name='L898' href='#L898'><pre>898</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L899' href='#L899'><pre>899</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>caches</span></pre></td></tr><tr><td class='line-number'><a name='L900' href='#L900'><pre>900</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>par_iter_mut</span>()</pre></td></tr><tr><td class='line-number'><a name='L901' href='#L901'><pre>901</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>zip</span>(<span class='region red'>prompts</span>.<span class='region red'>par_iter</span>())</pre></td></tr><tr><td class='line-number'><a name='L902' href='#L902'><pre>902</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>try_for_each</span>(|(cache, prompt)| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L903' href='#L903'><pre>903</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for (<span class='region red'>pos</span>, &amp;<span class='region red'>token_id</span>) in <span class='region red'>prompt.iter()</span>.<span class='region red'>enumerate</span>() {</pre></td></tr><tr><td class='line-number'><a name='L904' href='#L904'><pre>904</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model</span>.<span class='region red'>forward_single_with_cache</span>(<span class='region red'>token_id</span>, <span class='region red'>cache</span>, <span class='region red'>pos</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L905' href='#L905'><pre>905</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L906' href='#L906'><pre>906</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>Ok::&lt;_, RealizarError&gt;(())</span></pre></td></tr><tr><td class='line-number'><a name='L907' href='#L907'><pre>907</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L908' href='#L908'><pre>908</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L909' href='#L909'><pre>909</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Generation loop with batched FFN (PARITY-021: GPU optimization)</pre></td></tr><tr><td class='line-number'><a name='L910' href='#L910'><pre>910</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        for <span class='region red'>gen_idx</span> in 0..<span class='region red'>config.max_tokens</span> {</pre></td></tr><tr><td class='line-number'><a name='L911' href='#L911'><pre>911</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // Collect active prompts for this generation step</pre></td></tr><tr><td class='line-number'><a name='L912' href='#L912'><pre>912</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>active_indices</span>: <span class='region red'>Vec&lt;usize&gt;</span> = <span class='region red'>(0..num_prompts)</span>.<span class='region red'>filter</span>(|&amp;i| !<span class='region red'>done[i]</span>).<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L913' href='#L913'><pre>913</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L914' href='#L914'><pre>914</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            if <span class='region red'>active_indices</span>.<span class='region red'>is_empty</span>() {</pre></td></tr><tr><td class='line-number'><a name='L915' href='#L915'><pre>915</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>break</span>;</pre></td></tr><tr><td class='line-number'><a name='L916' href='#L916'><pre>916</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L917' href='#L917'><pre>917</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L918' href='#L918'><pre>918</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>active_count</span> = <span class='region red'>active_indices</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L919' href='#L919'><pre>919</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L920' href='#L920'><pre>920</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // Use batched forward when we have enough active prompts for GPU benefit</pre></td></tr><tr><td class='line-number'><a name='L921' href='#L921'><pre>921</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // GPU batch threshold is 32 (from IMP-600 analysis)</pre></td></tr><tr><td class='line-number'><a name='L922' href='#L922'><pre>922</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            const GPU_BATCH_THRESHOLD: usize = 32;</pre></td></tr><tr><td class='line-number'><a name='L923' href='#L923'><pre>923</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L924' href='#L924'><pre>924</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            if <span class='region red'>active_count &gt;= GPU_BATCH_THRESHOLD</span> {</pre></td></tr><tr><td class='line-number'><a name='L925' href='#L925'><pre>925</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // PARITY-021: Batched forward with GPU FFN</pre></td></tr><tr><td class='line-number'><a name='L926' href='#L926'><pre>926</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // Collect tokens, positions, and cache slices for active prompts</pre></td></tr><tr><td class='line-number'><a name='L927' href='#L927'><pre>927</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_tokens</span>: <span class='region red'>Vec&lt;u32&gt;</span> = <span class='region red'>active_indices</span></pre></td></tr><tr><td class='line-number'><a name='L928' href='#L928'><pre>928</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    .iter()</span></pre></td></tr><tr><td class='line-number'><a name='L929' href='#L929'><pre>929</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>map</span>(|&amp;idx| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L930' href='#L930'><pre>930</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>*</span><span class='region red'>sequences[idx]</span></pre></td></tr><tr><td class='line-number'><a name='L931' href='#L931'><pre>931</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                            .last()</span><span class='red'></span></pre></td></tr><tr><td class='line-number'><a name='L932' href='#L932'><pre>932</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                            .expect(&quot;sequence must have at least prompt tokens&quot;)</span></pre></td></tr><tr><td class='line-number'><a name='L933' href='#L933'><pre>933</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>}</span>)</pre></td></tr><tr><td class='line-number'><a name='L934' href='#L934'><pre>934</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L935' href='#L935'><pre>935</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L936' href='#L936'><pre>936</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_positions</span>: <span class='region red'>Vec&lt;usize&gt;</span> = <span class='region red'>active_indices</span></pre></td></tr><tr><td class='line-number'><a name='L937' href='#L937'><pre>937</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    .iter()</span></pre></td></tr><tr><td class='line-number'><a name='L938' href='#L938'><pre>938</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>map</span>(|&amp;idx| <span class='region red'>prompts[idx]</span>.<span class='region red'>len</span>() + <span class='region red'>gen_idx</span>)</pre></td></tr><tr><td class='line-number'><a name='L939' href='#L939'><pre>939</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L940' href='#L940'><pre>940</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L941' href='#L941'><pre>941</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // PARITY-096: Extract caches without cloning using std::mem::take</pre></td></tr><tr><td class='line-number'><a name='L942' href='#L942'><pre>942</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // This avoids expensive cache cloning on every generation step</pre></td></tr><tr><td class='line-number'><a name='L943' href='#L943'><pre>943</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>mut batch_caches</span>: <span class='region red'>Vec&lt;OwnedQuantizedKVCache&gt;</span> = <span class='region red'>active_indices</span></pre></td></tr><tr><td class='line-number'><a name='L944' href='#L944'><pre>944</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    .iter()</span></pre></td></tr><tr><td class='line-number'><a name='L945' href='#L945'><pre>945</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>map</span>(|&amp;idx| <span class='region red'>std::mem::take</span>(<span class='region red'>&amp;mut caches[idx]</span>))</pre></td></tr><tr><td class='line-number'><a name='L946' href='#L946'><pre>946</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L947' href='#L947'><pre>947</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L948' href='#L948'><pre>948</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // Forward batch with GPU FFN</pre></td></tr><tr><td class='line-number'><a name='L949' href='#L949'><pre>949</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>all_logits</span> = <span class='region red'>self</span>.<span class='region red'>forward_batch_with_gpu_ffn</span>(</pre></td></tr><tr><td class='line-number'><a name='L950' href='#L950'><pre>950</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>&amp;batch_tokens</span>,</pre></td></tr><tr><td class='line-number'><a name='L951' href='#L951'><pre>951</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>&amp;mut batch_caches</span>,</pre></td></tr><tr><td class='line-number'><a name='L952' href='#L952'><pre>952</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>&amp;batch_positions</span>,</pre></td></tr><tr><td class='line-number'><a name='L953' href='#L953'><pre>953</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L954' href='#L954'><pre>954</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L955' href='#L955'><pre>955</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // PARITY-096: Put caches back (move, not clone)</pre></td></tr><tr><td class='line-number'><a name='L956' href='#L956'><pre>956</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for (<span class='region red'>i</span>, &amp;<span class='region red'>idx</span>) in <span class='region red'>active_indices.iter()</span>.<span class='region red'>enumerate</span>() <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L957' href='#L957'><pre>957</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>caches[idx]</span><span class='region red'> = </span><span class='region red'>std::mem::take</span><span class='region red'>(&amp;mut batch_caches[i]);</span></pre></td></tr><tr><td class='line-number'><a name='L958' href='#L958'><pre>958</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                }</span></pre></td></tr><tr><td class='line-number'><a name='L959' href='#L959'><pre>959</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L960' href='#L960'><pre>960</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // Sample and update sequences</pre></td></tr><tr><td class='line-number'><a name='L961' href='#L961'><pre>961</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for (<span class='region red'>i</span>, &amp;<span class='region red'>prompt_idx</span>) in <span class='region red'>active_indices.iter()</span>.<span class='region red'>enumerate</span>() {</pre></td></tr><tr><td class='line-number'><a name='L962' href='#L962'><pre>962</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>logits</span> = <span class='region red'>&amp;all_logits[i]</span>;</pre></td></tr><tr><td class='line-number'><a name='L963' href='#L963'><pre>963</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>next_token</span> = if <span class='region red'>config.temperature == 0.0</span> || <span class='region red'>config.top_k == 1</span> {</pre></td></tr><tr><td class='line-number'><a name='L964' href='#L964'><pre>964</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>OwnedQuantizedModel::argmax</span>(<span class='region red'>logits</span>)</pre></td></tr><tr><td class='line-number'><a name='L965' href='#L965'><pre>965</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    } else {</pre></td></tr><tr><td class='line-number'><a name='L966' href='#L966'><pre>966</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>OwnedQuantizedModel::sample_topk</span>(<span class='region red'>logits</span>, <span class='region red'>config.temperature</span>, <span class='region red'>config.top_k</span>)</pre></td></tr><tr><td class='line-number'><a name='L967' href='#L967'><pre>967</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    };</pre></td></tr><tr><td class='line-number'><a name='L968' href='#L968'><pre>968</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L969' href='#L969'><pre>969</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    if <span class='region red'>config.stop_tokens</span><span class='region red'>.contains(&amp;next_token)</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L970' href='#L970'><pre>970</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        done[prompt_idx] = true;</span></pre></td></tr><tr><td class='line-number'><a name='L971' href='#L971'><pre>971</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    }</span> else {</pre></td></tr><tr><td class='line-number'><a name='L972' href='#L972'><pre>972</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>sequences[prompt_idx]</span>.<span class='region red'>push</span>(<span class='region red'>next_token</span>);</pre></td></tr><tr><td class='line-number'><a name='L973' href='#L973'><pre>973</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        if <span class='region red'>sequences[prompt_idx]</span><span class='region red'>.len() &gt;= max_seq_len</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L974' href='#L974'><pre>974</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                            done[prompt_idx] = true;</span></pre></td></tr><tr><td class='line-number'><a name='L975' href='#L975'><pre>975</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L976' href='#L976'><pre>976</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    }</pre></td></tr><tr><td class='line-number'><a name='L977' href='#L977'><pre>977</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L978' href='#L978'><pre>978</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            } else {</pre></td></tr><tr><td class='line-number'><a name='L979' href='#L979'><pre>979</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // Sequential forward for small batches (CPU is faster)</pre></td></tr><tr><td class='line-number'><a name='L980' href='#L980'><pre>980</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for &amp;<span class='region red'>prompt_idx</span> in <span class='region red'>&amp;active_indices</span> {</pre></td></tr><tr><td class='line-number'><a name='L981' href='#L981'><pre>981</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>position</span> = <span class='region red'>prompts[prompt_idx]</span><span class='region red'>.len() + gen_idx</span>;</pre></td></tr><tr><td class='line-number'><a name='L982' href='#L982'><pre>982</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>last_token</span> = <span class='region red'>*</span><span class='region red'>sequences[prompt_idx]</span></pre></td></tr><tr><td class='line-number'><a name='L983' href='#L983'><pre>983</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        .last()</span><span class='red'></span></pre></td></tr><tr><td class='line-number'><a name='L984' href='#L984'><pre>984</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        .expect(&quot;sequence must have at least prompt tokens&quot;)</span>;</pre></td></tr><tr><td class='line-number'><a name='L985' href='#L985'><pre>985</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L986' href='#L986'><pre>986</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>logits</span> = <span class='region red'>self.model</span>.<span class='region red'>forward_single_with_cache</span>(</pre></td></tr><tr><td class='line-number'><a name='L987' href='#L987'><pre>987</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>last_token</span>,</pre></td></tr><tr><td class='line-number'><a name='L988' href='#L988'><pre>988</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>&amp;mut caches[prompt_idx]</span>,</pre></td></tr><tr><td class='line-number'><a name='L989' href='#L989'><pre>989</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>position</span>,</pre></td></tr><tr><td class='line-number'><a name='L990' href='#L990'><pre>990</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    )<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L991' href='#L991'><pre>991</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L992' href='#L992'><pre>992</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>next_token</span> = if <span class='region red'>config.temperature == 0.0</span> || <span class='region red'>config.top_k == 1</span> {</pre></td></tr><tr><td class='line-number'><a name='L993' href='#L993'><pre>993</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>OwnedQuantizedModel::argmax</span>(<span class='region red'>&amp;logits</span>)</pre></td></tr><tr><td class='line-number'><a name='L994' href='#L994'><pre>994</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    } else {</pre></td></tr><tr><td class='line-number'><a name='L995' href='#L995'><pre>995</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>OwnedQuantizedModel::sample_topk</span>(<span class='region red'>&amp;logits</span>, <span class='region red'>config.temperature</span>, <span class='region red'>config.top_k</span>)</pre></td></tr><tr><td class='line-number'><a name='L996' href='#L996'><pre>996</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    };</pre></td></tr><tr><td class='line-number'><a name='L997' href='#L997'><pre>997</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L998' href='#L998'><pre>998</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    if <span class='region red'>config.stop_tokens</span><span class='region red'>.contains(&amp;next_token)</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L999' href='#L999'><pre>999</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        done[prompt_idx] = true;</span></pre></td></tr><tr><td class='line-number'><a name='L1000' href='#L1000'><pre>1000</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    }</span> else {</pre></td></tr><tr><td class='line-number'><a name='L1001' href='#L1001'><pre>1001</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>sequences[prompt_idx]</span>.<span class='region red'>push</span>(<span class='region red'>next_token</span>);</pre></td></tr><tr><td class='line-number'><a name='L1002' href='#L1002'><pre>1002</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        if <span class='region red'>sequences[prompt_idx]</span><span class='region red'>.len() &gt;= max_seq_len</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1003' href='#L1003'><pre>1003</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                            done[prompt_idx] = true;</span></pre></td></tr><tr><td class='line-number'><a name='L1004' href='#L1004'><pre>1004</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1005' href='#L1005'><pre>1005</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    }</pre></td></tr><tr><td class='line-number'><a name='L1006' href='#L1006'><pre>1006</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L1007' href='#L1007'><pre>1007</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L1008' href='#L1008'><pre>1008</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L1009' href='#L1009'><pre>1009</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1010' href='#L1010'><pre>1010</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(sequences)</span></pre></td></tr><tr><td class='line-number'><a name='L1011' href='#L1011'><pre>1011</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1012' href='#L1012'><pre>1012</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1013' href='#L1013'><pre>1013</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Batched forward pass with GPU FFN optimization (PARITY-021)</pre></td></tr><tr><td class='line-number'><a name='L1014' href='#L1014'><pre>1014</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L1015' href='#L1015'><pre>1015</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Processes multiple tokens in parallel with GPU-accelerated FFN.</pre></td></tr><tr><td class='line-number'><a name='L1016' href='#L1016'><pre>1016</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Attention is still per-token with CPU KV cache, but FFN uses GPU GEMM.</pre></td></tr><tr><td class='line-number'><a name='L1017' href='#L1017'><pre>1017</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L1018' href='#L1018'><pre>1018</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Arguments</pre></td></tr><tr><td class='line-number'><a name='L1019' href='#L1019'><pre>1019</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `token_ids` - Token IDs for each prompt [batch_size]</pre></td></tr><tr><td class='line-number'><a name='L1020' href='#L1020'><pre>1020</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `caches` - Per-prompt KV caches</pre></td></tr><tr><td class='line-number'><a name='L1021' href='#L1021'><pre>1021</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// * `positions` - Position for each prompt [batch_size]</pre></td></tr><tr><td class='line-number'><a name='L1022' href='#L1022'><pre>1022</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L1023' href='#L1023'><pre>1023</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # Returns</pre></td></tr><tr><td class='line-number'><a name='L1024' href='#L1024'><pre>1024</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Logits for each prompt [batch_size][vocab_size]</pre></td></tr><tr><td class='line-number'><a name='L1025' href='#L1025'><pre>1025</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L1026' href='#L1026'><pre>1026</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// # GPU Dispatch</pre></td></tr><tr><td class='line-number'><a name='L1027' href='#L1027'><pre>1027</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - batch_size &gt;= 32: GPU GEMM for FFN (10x speedup)</pre></td></tr><tr><td class='line-number'><a name='L1028' href='#L1028'><pre>1028</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// - batch_size &lt; 32: CPU fallback</pre></td></tr><tr><td class='line-number'><a name='L1029' href='#L1029'><pre>1029</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn forward_batch_with_gpu_ffn(</span></pre></td></tr><tr><td class='line-number'><a name='L1030' href='#L1030'><pre>1030</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        &amp;self,</span></pre></td></tr><tr><td class='line-number'><a name='L1031' href='#L1031'><pre>1031</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        token_ids: &amp;[u32],</span></pre></td></tr><tr><td class='line-number'><a name='L1032' href='#L1032'><pre>1032</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        caches: &amp;mut [OwnedQuantizedKVCache],</span></pre></td></tr><tr><td class='line-number'><a name='L1033' href='#L1033'><pre>1033</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        positions: &amp;[usize],</span></pre></td></tr><tr><td class='line-number'><a name='L1034' href='#L1034'><pre>1034</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>    ) -&gt; Result&lt;Vec&lt;Vec&lt;f32&gt;&gt;&gt;</span> {</pre></td></tr><tr><td class='line-number'><a name='L1035' href='#L1035'><pre>1035</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>batch_size</span> = <span class='region red'>token_ids</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L1036' href='#L1036'><pre>1036</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>batch_size == 0</span> {</pre></td></tr><tr><td class='line-number'><a name='L1037' href='#L1037'><pre>1037</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Ok(Vec::new())</span>;</pre></td></tr><tr><td class='line-number'><a name='L1038' href='#L1038'><pre>1038</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1039' href='#L1039'><pre>1039</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        if <span class='region red'>batch_size</span><span class='region red'> != caches.len()</span> || <span class='region red'>batch_size</span><span class='region red'> != positions.len()</span> {</pre></td></tr><tr><td class='line-number'><a name='L1040' href='#L1040'><pre>1040</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            return <span class='region red'>Err(RealizarError::InvalidShape {</span></pre></td></tr><tr><td class='line-number'><a name='L1041' href='#L1041'><pre>1041</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                reason: </span><span class='region red'>format!</span><span class='region red'>(</span></pre></td></tr><tr><td class='line-number'><a name='L1042' href='#L1042'><pre>1042</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>&quot;Batch size mismatch: tokens={}, caches={}, positions={}&quot;</span><span class='region red'>,</span></pre></td></tr><tr><td class='line-number'><a name='L1043' href='#L1043'><pre>1043</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    batch_size,</span></pre></td></tr><tr><td class='line-number'><a name='L1044' href='#L1044'><pre>1044</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>caches</span><span class='region red'>.</span><span class='region red'>len</span><span class='region red'>(),</span></pre></td></tr><tr><td class='line-number'><a name='L1045' href='#L1045'><pre>1045</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    positions.len()</span></pre></td></tr><tr><td class='line-number'><a name='L1046' href='#L1046'><pre>1046</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                ),</span></pre></td></tr><tr><td class='line-number'><a name='L1047' href='#L1047'><pre>1047</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            })</span>;</pre></td></tr><tr><td class='line-number'><a name='L1048' href='#L1048'><pre>1048</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1049' href='#L1049'><pre>1049</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1050' href='#L1050'><pre>1050</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>self.model.config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1051' href='#L1051'><pre>1051</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>num_layers</span> = <span class='region red'>self.model.layers</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L1052' href='#L1052'><pre>1052</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1053' href='#L1053'><pre>1053</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // Threshold for GPU dispatch (based on IMP-600 analysis)</pre></td></tr><tr><td class='line-number'><a name='L1054' href='#L1054'><pre>1054</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        const GPU_BATCH_THRESHOLD: usize = 32;</pre></td></tr><tr><td class='line-number'><a name='L1055' href='#L1055'><pre>1055</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>use_gpu</span> = <span class='region red'>batch_size &gt;= GPU_BATCH_THRESHOLD</span> &amp;&amp; <span class='region red'>self</span>.<span class='region red'>is_gpu_cache_warm</span>();</pre></td></tr><tr><td class='line-number'><a name='L1056' href='#L1056'><pre>1056</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1057' href='#L1057'><pre>1057</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-098: Parallel embedding using rayon</pre></td></tr><tr><td class='line-number'><a name='L1058' href='#L1058'><pre>1058</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        use rayon::prelude::*;</pre></td></tr><tr><td class='line-number'><a name='L1059' href='#L1059'><pre>1059</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>mut hidden_states</span>: <span class='region red'>Vec&lt;Vec&lt;f32&gt;&gt;</span> = <span class='region red'>token_ids</span></pre></td></tr><tr><td class='line-number'><a name='L1060' href='#L1060'><pre>1060</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>par_iter</span>()</pre></td></tr><tr><td class='line-number'><a name='L1061' href='#L1061'><pre>1061</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>map</span>(|&amp;tid| <span class='region red'>self.model</span>.<span class='region red'>embed</span>(<span class='region red'>&amp;[tid]</span>))</pre></td></tr><tr><td class='line-number'><a name='L1062' href='#L1062'><pre>1062</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1063' href='#L1063'><pre>1063</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1064' href='#L1064'><pre>1064</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // 2. Process through transformer layers</pre></td></tr><tr><td class='line-number'><a name='L1065' href='#L1065'><pre>1065</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        for <span class='region red'>layer_idx</span> in 0..<span class='region red'>num_layers</span> {</pre></td></tr><tr><td class='line-number'><a name='L1066' href='#L1066'><pre>1066</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>layer</span> = <span class='region red'>&amp;self.model.layers[layer_idx]</span>;</pre></td></tr><tr><td class='line-number'><a name='L1067' href='#L1067'><pre>1067</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1068' href='#L1068'><pre>1068</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // PARITY-024: GPU batch attention path vs CPU sequential path</pre></td></tr><tr><td class='line-number'><a name='L1069' href='#L1069'><pre>1069</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            if <span class='region red'>use_gpu</span> {</pre></td></tr><tr><td class='line-number'><a name='L1070' href='#L1070'><pre>1070</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // GPU path: batch QKV projection, per-prompt attention, batch output projection</pre></td></tr><tr><td class='line-number'><a name='L1071' href='#L1071'><pre>1071</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1072' href='#L1072'><pre>1072</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // 2a. PARITY-098: Parallel batch layer norm</pre></td></tr><tr><td class='line-number'><a name='L1073' href='#L1073'><pre>1073</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>normed_batch</span>: <span class='region red'>Vec&lt;Vec&lt;f32&gt;&gt;</span> = <span class='region red'>hidden_states</span></pre></td></tr><tr><td class='line-number'><a name='L1074' href='#L1074'><pre>1074</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>par_iter</span>()</pre></td></tr><tr><td class='line-number'><a name='L1075' href='#L1075'><pre>1075</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>map</span>(|hidden| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1076' href='#L1076'><pre>1076</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model</span>.<span class='region red'>layer_norm</span>(</pre></td></tr><tr><td class='line-number'><a name='L1077' href='#L1077'><pre>1077</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>hidden</span>,</pre></td></tr><tr><td class='line-number'><a name='L1078' href='#L1078'><pre>1078</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>&amp;layer.attn_norm_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L1079' href='#L1079'><pre>1079</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>layer.attn_norm_bias</span>.<span class='region red'>as_deref</span>(),</pre></td></tr><tr><td class='line-number'><a name='L1080' href='#L1080'><pre>1080</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>self.model.config.eps</span>,</pre></td></tr><tr><td class='line-number'><a name='L1081' href='#L1081'><pre>1081</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        )</pre></td></tr><tr><td class='line-number'><a name='L1082' href='#L1082'><pre>1082</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>}</span>)</pre></td></tr><tr><td class='line-number'><a name='L1083' href='#L1083'><pre>1083</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1084' href='#L1084'><pre>1084</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1085' href='#L1085'><pre>1085</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // 2b. Batch QKV projection using GPU GEMM (PARITY-024)</pre></td></tr><tr><td class='line-number'><a name='L1086' href='#L1086'><pre>1086</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_normed</span>: <span class='region red'>Vec&lt;f32&gt;</span> = <span class='region red'>normed_batch.iter()</span>.<span class='region red'>flatten</span>().<span class='region red'>copied</span>().<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1087' href='#L1087'><pre>1087</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_qkv</span> = <span class='region red'>self</span>.<span class='region red'>batch_qkv_projection_gpu</span>(<span class='region red'>&amp;batch_normed</span>, <span class='region red'>layer_idx</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1088' href='#L1088'><pre>1088</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1089' href='#L1089'><pre>1089</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // 2c-2e. PARITY-099: Parallel attention computation per prompt</pre></td></tr><tr><td class='line-number'><a name='L1090' href='#L1090'><pre>1090</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // Each prompt has its own KV cache, so we can parallelize</pre></td></tr><tr><td class='line-number'><a name='L1091' href='#L1091'><pre>1091</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>qkv_dim</span> = <span class='region red'>3 * hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1092' href='#L1092'><pre>1092</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1093' href='#L1093'><pre>1093</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>attention_outputs</span>: <span class='region red'>Vec&lt;Vec&lt;f32&gt;&gt;</span> = <span class='region red'>caches</span></pre></td></tr><tr><td class='line-number'><a name='L1094' href='#L1094'><pre>1094</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>par_iter_mut</span>()</pre></td></tr><tr><td class='line-number'><a name='L1095' href='#L1095'><pre>1095</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>enumerate</span>()</pre></td></tr><tr><td class='line-number'><a name='L1096' href='#L1096'><pre>1096</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>map</span>(|(prompt_idx, cache)| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1097' href='#L1097'><pre>1097</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>qkv_start</span> = <span class='region red'>prompt_idx * qkv_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1098' href='#L1098'><pre>1098</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>qkv</span> = <span class='region red'>&amp;</span><span class='region red'>batch_qkv</span><span class='region red'>[</span><span class='region red'>qkv_start</span><span class='region red'>..qkv_start + qkv_dim]</span>;</pre></td></tr><tr><td class='line-number'><a name='L1099' href='#L1099'><pre>1099</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1100' href='#L1100'><pre>1100</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // Extract Q, K, V</pre></td></tr><tr><td class='line-number'><a name='L1101' href='#L1101'><pre>1101</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>mut q</span> = <span class='region red'>qkv[0..hidden_dim]</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L1102' href='#L1102'><pre>1102</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>mut k</span> = <span class='region red'>qkv</span><span class='region red'>[hidden_dim..2 * hidden_dim]</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L1103' href='#L1103'><pre>1103</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>v</span> = <span class='region red'>qkv</span><span class='region red'>[2 * hidden_dim..3 * hidden_dim]</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L1104' href='#L1104'><pre>1104</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1105' href='#L1105'><pre>1105</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // Apply RoPE (position-dependent, must be per-prompt)</pre></td></tr><tr><td class='line-number'><a name='L1106' href='#L1106'><pre>1106</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // Note: Uses num_heads for both (non-GQA code path)</pre></td></tr><tr><td class='line-number'><a name='L1107' href='#L1107'><pre>1107</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model</span>.<span class='region red'>apply_rope</span>(</pre></td></tr><tr><td class='line-number'><a name='L1108' href='#L1108'><pre>1108</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>&amp;mut q</span>,</pre></td></tr><tr><td class='line-number'><a name='L1109' href='#L1109'><pre>1109</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>positions[prompt_idx]</span>,</pre></td></tr><tr><td class='line-number'><a name='L1110' href='#L1110'><pre>1110</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>self.model.config.num_heads</span>,</pre></td></tr><tr><td class='line-number'><a name='L1111' href='#L1111'><pre>1111</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        );</pre></td></tr><tr><td class='line-number'><a name='L1112' href='#L1112'><pre>1112</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model</span>.<span class='region red'>apply_rope</span>(</pre></td></tr><tr><td class='line-number'><a name='L1113' href='#L1113'><pre>1113</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>&amp;mut k</span>,</pre></td></tr><tr><td class='line-number'><a name='L1114' href='#L1114'><pre>1114</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>positions[prompt_idx]</span>,</pre></td></tr><tr><td class='line-number'><a name='L1115' href='#L1115'><pre>1115</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>self.model.config.num_heads</span>,</pre></td></tr><tr><td class='line-number'><a name='L1116' href='#L1116'><pre>1116</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        );</pre></td></tr><tr><td class='line-number'><a name='L1117' href='#L1117'><pre>1117</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1118' href='#L1118'><pre>1118</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // Attention with KV cache (must be per-prompt, different caches)</pre></td></tr><tr><td class='line-number'><a name='L1119' href='#L1119'><pre>1119</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // PARITY-027: Use FlashAttention for long sequences (O(N) memory)</pre></td></tr><tr><td class='line-number'><a name='L1120' href='#L1120'><pre>1120</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>k_cache</span> = <span class='region red'>cache</span>.<span class='region red'>get_k</span>(<span class='region red'>layer_idx</span>);</pre></td></tr><tr><td class='line-number'><a name='L1121' href='#L1121'><pre>1121</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>v_cache</span> = <span class='region red'>cache</span>.<span class='region red'>get_v</span>(<span class='region red'>layer_idx</span>);</pre></td></tr><tr><td class='line-number'><a name='L1122' href='#L1122'><pre>1122</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1123' href='#L1123'><pre>1123</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // FlashAttention threshold: use for sequences &gt;= 512 tokens</pre></td></tr><tr><td class='line-number'><a name='L1124' href='#L1124'><pre>1124</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        const FLASH_ATTENTION_THRESHOLD: usize = 512;</pre></td></tr><tr><td class='line-number'><a name='L1125' href='#L1125'><pre>1125</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>cache_len</span> = <span class='region red'>k_cache</span><span class='region red'>.len() / hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1126' href='#L1126'><pre>1126</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>use_flash_attention</span> = <span class='region red'>cache_len &gt;= FLASH_ATTENTION_THRESHOLD</span>;</pre></td></tr><tr><td class='line-number'><a name='L1127' href='#L1127'><pre>1127</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1128' href='#L1128'><pre>1128</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>attn_out</span> = if <span class='region red'>k_cache</span>.<span class='region red'>is_empty</span>() {</pre></td></tr><tr><td class='line-number'><a name='L1129' href='#L1129'><pre>1129</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>v</span>.<span class='region red'>clone</span>()</pre></td></tr><tr><td class='line-number'><a name='L1130' href='#L1130'><pre>1130</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        } else if <span class='region red'>use_flash_attention</span> {</pre></td></tr><tr><td class='line-number'><a name='L1131' href='#L1131'><pre>1131</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                            // FlashAttention: O(N) memory, tiled computation</pre></td></tr><tr><td class='line-number'><a name='L1132' href='#L1132'><pre>1132</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                            const FLASH_BLOCK_SIZE: usize = 64;</pre></td></tr><tr><td class='line-number'><a name='L1133' href='#L1133'><pre>1133</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>self.model</span>.<span class='region red'>flash_attention_tiled</span>(</pre></td></tr><tr><td class='line-number'><a name='L1134' href='#L1134'><pre>1134</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                                <span class='region red'>&amp;q</span>,</pre></td></tr><tr><td class='line-number'><a name='L1135' href='#L1135'><pre>1135</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                                <span class='region red'>k_cache</span>,</pre></td></tr><tr><td class='line-number'><a name='L1136' href='#L1136'><pre>1136</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                                <span class='region red'>v_cache</span>,</pre></td></tr><tr><td class='line-number'><a name='L1137' href='#L1137'><pre>1137</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                                <span class='region red'>&amp;k</span>,</pre></td></tr><tr><td class='line-number'><a name='L1138' href='#L1138'><pre>1138</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                                <span class='region red'>&amp;v</span>,</pre></td></tr><tr><td class='line-number'><a name='L1139' href='#L1139'><pre>1139</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                                FLASH_BLOCK_SIZE,</pre></td></tr><tr><td class='line-number'><a name='L1140' href='#L1140'><pre>1140</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                            )</pre></td></tr><tr><td class='line-number'><a name='L1141' href='#L1141'><pre>1141</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        } else {</pre></td></tr><tr><td class='line-number'><a name='L1142' href='#L1142'><pre>1142</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                            // Standard attention: O(N) memory but faster for short sequences</pre></td></tr><tr><td class='line-number'><a name='L1143' href='#L1143'><pre>1143</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            <span class='region red'>self.model</span></pre></td></tr><tr><td class='line-number'><a name='L1144' href='#L1144'><pre>1144</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                                .<span class='region red'>attention_with_cache</span>(<span class='region red'>&amp;q</span>, <span class='region red'>k_cache</span>, <span class='region red'>v_cache</span>, <span class='region red'>&amp;k</span>, <span class='region red'>&amp;v</span>)</pre></td></tr><tr><td class='line-number'><a name='L1145' href='#L1145'><pre>1145</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        };</pre></td></tr><tr><td class='line-number'><a name='L1146' href='#L1146'><pre>1146</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1147' href='#L1147'><pre>1147</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                        // Store K and V in cache</pre></td></tr><tr><td class='line-number'><a name='L1148' href='#L1148'><pre>1148</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>cache</span>.<span class='region red'>append</span>(<span class='region red'>layer_idx</span>, <span class='region red'>&amp;k</span>, <span class='region red'>&amp;v</span>);</pre></td></tr><tr><td class='line-number'><a name='L1149' href='#L1149'><pre>1149</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>attn_out</span></pre></td></tr><tr><td class='line-number'><a name='L1150' href='#L1150'><pre>1150</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>}</span>)</pre></td></tr><tr><td class='line-number'><a name='L1151' href='#L1151'><pre>1151</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1152' href='#L1152'><pre>1152</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1153' href='#L1153'><pre>1153</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // 2f. Batch attention output projection using GPU GEMM (PARITY-024)</pre></td></tr><tr><td class='line-number'><a name='L1154' href='#L1154'><pre>1154</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_attn</span>: <span class='region red'>Vec&lt;f32&gt;</span> = <span class='region red'>attention_outputs.iter()</span>.<span class='region red'>flatten</span>().<span class='region red'>copied</span>().<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1155' href='#L1155'><pre>1155</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_output</span> = <span class='region red'>self</span>.<span class='region red'>batch_attention_output_gpu</span>(<span class='region red'>&amp;batch_attn</span>, <span class='region red'>layer_idx</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1156' href='#L1156'><pre>1156</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1157' href='#L1157'><pre>1157</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // 2g. PARITY-100: Parallel residual connection</pre></td></tr><tr><td class='line-number'><a name='L1158' href='#L1158'><pre>1158</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>hidden_states</span></pre></td></tr><tr><td class='line-number'><a name='L1159' href='#L1159'><pre>1159</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>par_iter_mut</span>()</pre></td></tr><tr><td class='line-number'><a name='L1160' href='#L1160'><pre>1160</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>enumerate</span>()</pre></td></tr><tr><td class='line-number'><a name='L1161' href='#L1161'><pre>1161</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>for_each</span>(|(prompt_idx, hidden)| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1162' href='#L1162'><pre>1162</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>start</span> = <span class='region red'>prompt_idx * hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1163' href='#L1163'><pre>1163</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        for <span class='region red'>i</span> in 0..<span class='region red'>hidden_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1164' href='#L1164'><pre>1164</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                            </span><span class='region red'>hidden[i]</span><span class='region red'> += </span><span class='region red'>batch_output</span><span class='region red'>[start + i];</span></pre></td></tr><tr><td class='line-number'><a name='L1165' href='#L1165'><pre>1165</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        }</span></pre></td></tr><tr><td class='line-number'><a name='L1166' href='#L1166'><pre>1166</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>}</span>);</pre></td></tr><tr><td class='line-number'><a name='L1167' href='#L1167'><pre>1167</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            } else {</pre></td></tr><tr><td class='line-number'><a name='L1168' href='#L1168'><pre>1168</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // CPU sequential path (original implementation)</pre></td></tr><tr><td class='line-number'><a name='L1169' href='#L1169'><pre>1169</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for (<span class='region red'>prompt_idx</span>, <span class='region red'>hidden</span>) in <span class='region red'>hidden_states.iter_mut()</span>.<span class='region red'>enumerate</span>() {</pre></td></tr><tr><td class='line-number'><a name='L1170' href='#L1170'><pre>1170</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Attention layer norm</pre></td></tr><tr><td class='line-number'><a name='L1171' href='#L1171'><pre>1171</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>normed</span> = <span class='region red'>self.model</span>.<span class='region red'>layer_norm</span>(</pre></td></tr><tr><td class='line-number'><a name='L1172' href='#L1172'><pre>1172</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>hidden</span>,</pre></td></tr><tr><td class='line-number'><a name='L1173' href='#L1173'><pre>1173</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>&amp;layer.attn_norm_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L1174' href='#L1174'><pre>1174</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>layer.attn_norm_bias</span>.<span class='region red'>as_deref</span>(),</pre></td></tr><tr><td class='line-number'><a name='L1175' href='#L1175'><pre>1175</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model.config.eps</span>,</pre></td></tr><tr><td class='line-number'><a name='L1176' href='#L1176'><pre>1176</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    );</pre></td></tr><tr><td class='line-number'><a name='L1177' href='#L1177'><pre>1177</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1178' href='#L1178'><pre>1178</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // QKV projection</pre></td></tr><tr><td class='line-number'><a name='L1179' href='#L1179'><pre>1179</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>mut qkv</span> = <span class='region red'>self.model</span>.<span class='region red'>qkv_matmul</span>(<span class='region red'>&amp;normed</span>, <span class='region red'>&amp;layer.qkv_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1180' href='#L1180'><pre>1180</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>layer.qkv_bias</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1181' href='#L1181'><pre>1181</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>self.model</span><span class='region red'>.</span><span class='region red'>add_bias</span><span class='region red'>(&amp;mut qkv, bias);</span></pre></td></tr><tr><td class='line-number'><a name='L1182' href='#L1182'><pre>1182</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1183' href='#L1183'><pre>1183</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1184' href='#L1184'><pre>1184</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Extract Q, K, V and apply RoPE</pre></td></tr><tr><td class='line-number'><a name='L1185' href='#L1185'><pre>1185</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Note: Uses num_heads for both (non-GQA code path)</pre></td></tr><tr><td class='line-number'><a name='L1186' href='#L1186'><pre>1186</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>mut q</span> = <span class='region red'>qkv[0..hidden_dim]</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L1187' href='#L1187'><pre>1187</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>mut k</span> = <span class='region red'>qkv</span><span class='region red'>[hidden_dim..2 * hidden_dim]</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L1188' href='#L1188'><pre>1188</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>v</span> = <span class='region red'>qkv</span><span class='region red'>[2 * hidden_dim..3 * hidden_dim]</span>.<span class='region red'>to_vec</span>();</pre></td></tr><tr><td class='line-number'><a name='L1189' href='#L1189'><pre>1189</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1190' href='#L1190'><pre>1190</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model</span>.<span class='region red'>apply_rope</span>(</pre></td></tr><tr><td class='line-number'><a name='L1191' href='#L1191'><pre>1191</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>&amp;mut q</span>,</pre></td></tr><tr><td class='line-number'><a name='L1192' href='#L1192'><pre>1192</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>positions[prompt_idx]</span>,</pre></td></tr><tr><td class='line-number'><a name='L1193' href='#L1193'><pre>1193</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model.config.num_heads</span>,</pre></td></tr><tr><td class='line-number'><a name='L1194' href='#L1194'><pre>1194</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    );</pre></td></tr><tr><td class='line-number'><a name='L1195' href='#L1195'><pre>1195</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model</span>.<span class='region red'>apply_rope</span>(</pre></td></tr><tr><td class='line-number'><a name='L1196' href='#L1196'><pre>1196</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>&amp;mut k</span>,</pre></td></tr><tr><td class='line-number'><a name='L1197' href='#L1197'><pre>1197</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>positions[prompt_idx]</span>,</pre></td></tr><tr><td class='line-number'><a name='L1198' href='#L1198'><pre>1198</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model.config.num_heads</span>,</pre></td></tr><tr><td class='line-number'><a name='L1199' href='#L1199'><pre>1199</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    );</pre></td></tr><tr><td class='line-number'><a name='L1200' href='#L1200'><pre>1200</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1201' href='#L1201'><pre>1201</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Get cached K/V and compute attention</pre></td></tr><tr><td class='line-number'><a name='L1202' href='#L1202'><pre>1202</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>k_cache</span> = <span class='region red'>caches[prompt_idx]</span>.<span class='region red'>get_k</span>(<span class='region red'>layer_idx</span>);</pre></td></tr><tr><td class='line-number'><a name='L1203' href='#L1203'><pre>1203</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>v_cache</span> = <span class='region red'>caches[prompt_idx]</span>.<span class='region red'>get_v</span>(<span class='region red'>layer_idx</span>);</pre></td></tr><tr><td class='line-number'><a name='L1204' href='#L1204'><pre>1204</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1205' href='#L1205'><pre>1205</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>attn_out</span> = if <span class='region red'>k_cache</span>.<span class='region red'>is_empty</span>() {</pre></td></tr><tr><td class='line-number'><a name='L1206' href='#L1206'><pre>1206</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>v</span>.<span class='region red'>clone</span>()</pre></td></tr><tr><td class='line-number'><a name='L1207' href='#L1207'><pre>1207</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    } else {</pre></td></tr><tr><td class='line-number'><a name='L1208' href='#L1208'><pre>1208</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model</span></pre></td></tr><tr><td class='line-number'><a name='L1209' href='#L1209'><pre>1209</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                            .<span class='region red'>attention_with_cache</span>(<span class='region red'>&amp;q</span>, <span class='region red'>k_cache</span>, <span class='region red'>v_cache</span>, <span class='region red'>&amp;k</span>, <span class='region red'>&amp;v</span>)</pre></td></tr><tr><td class='line-number'><a name='L1210' href='#L1210'><pre>1210</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    };</pre></td></tr><tr><td class='line-number'><a name='L1211' href='#L1211'><pre>1211</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1212' href='#L1212'><pre>1212</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Store K and V in cache</pre></td></tr><tr><td class='line-number'><a name='L1213' href='#L1213'><pre>1213</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>caches[prompt_idx]</span>.<span class='region red'>append</span>(<span class='region red'>layer_idx</span>, <span class='region red'>&amp;k</span>, <span class='region red'>&amp;v</span>);</pre></td></tr><tr><td class='line-number'><a name='L1214' href='#L1214'><pre>1214</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1215' href='#L1215'><pre>1215</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Attention output projection</pre></td></tr><tr><td class='line-number'><a name='L1216' href='#L1216'><pre>1216</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>mut attn_output</span> = <span class='region red'>self</span></pre></td></tr><tr><td class='line-number'><a name='L1217' href='#L1217'><pre>1217</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        .model</span></pre></td></tr><tr><td class='line-number'><a name='L1218' href='#L1218'><pre>1218</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        .<span class='region red'>fused_matmul</span>(<span class='region red'>&amp;attn_out</span>, <span class='region red'>&amp;layer.attn_output_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1219' href='#L1219'><pre>1219</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>layer.attn_output_bias</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1220' href='#L1220'><pre>1220</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>self.model</span><span class='region red'>.</span><span class='region red'>add_bias</span><span class='region red'>(&amp;mut attn_output, bias);</span></pre></td></tr><tr><td class='line-number'><a name='L1221' href='#L1221'><pre>1221</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1222' href='#L1222'><pre>1222</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1223' href='#L1223'><pre>1223</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Residual connection</pre></td></tr><tr><td class='line-number'><a name='L1224' href='#L1224'><pre>1224</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    for <span class='region red'>i</span> in 0..<span class='region red'>hidden_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1225' href='#L1225'><pre>1225</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>hidden[i]</span><span class='region red'> += attn_output[i];</span></pre></td></tr><tr><td class='line-number'><a name='L1226' href='#L1226'><pre>1226</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    }</span></pre></td></tr><tr><td class='line-number'><a name='L1227' href='#L1227'><pre>1227</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L1228' href='#L1228'><pre>1228</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L1229' href='#L1229'><pre>1229</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1230' href='#L1230'><pre>1230</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // 2h. FFN - GPU batch or CPU sequential</pre></td></tr><tr><td class='line-number'><a name='L1231' href='#L1231'><pre>1231</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            if <span class='region red'>use_gpu</span> {</pre></td></tr><tr><td class='line-number'><a name='L1232' href='#L1232'><pre>1232</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // GPU batch FFN: collect hidden states, process together, scatter back</pre></td></tr><tr><td class='line-number'><a name='L1233' href='#L1233'><pre>1233</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>batch_hidden</span>: <span class='region red'>Vec&lt;f32&gt;</span> = <span class='region red'>hidden_states.iter()</span>.<span class='region red'>flatten</span>().<span class='region red'>copied</span>().<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1234' href='#L1234'><pre>1234</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>ffn_output</span> = <span class='region red'>self</span>.<span class='region red'>batch_ffn_gpu</span>(<span class='region red'>&amp;batch_hidden</span>, <span class='region red'>layer_idx</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1235' href='#L1235'><pre>1235</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1236' href='#L1236'><pre>1236</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // PARITY-100: Parallel scatter and residual</pre></td></tr><tr><td class='line-number'><a name='L1237' href='#L1237'><pre>1237</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>hidden_states</span></pre></td></tr><tr><td class='line-number'><a name='L1238' href='#L1238'><pre>1238</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>par_iter_mut</span>()</pre></td></tr><tr><td class='line-number'><a name='L1239' href='#L1239'><pre>1239</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>enumerate</span>()</pre></td></tr><tr><td class='line-number'><a name='L1240' href='#L1240'><pre>1240</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>for_each</span>(|(prompt_idx, hidden)| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1241' href='#L1241'><pre>1241</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        let <span class='region red'>start</span> = <span class='region red'>prompt_idx * hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1242' href='#L1242'><pre>1242</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        for <span class='region red'>i</span> in 0..<span class='region red'>hidden_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1243' href='#L1243'><pre>1243</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                            </span><span class='region red'>hidden[i]</span><span class='region red'> += </span><span class='region red'>ffn_output</span><span class='region red'>[start + i];</span></pre></td></tr><tr><td class='line-number'><a name='L1244' href='#L1244'><pre>1244</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        }</span></pre></td></tr><tr><td class='line-number'><a name='L1245' href='#L1245'><pre>1245</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>}</span>);</pre></td></tr><tr><td class='line-number'><a name='L1246' href='#L1246'><pre>1246</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            } else {</pre></td></tr><tr><td class='line-number'><a name='L1247' href='#L1247'><pre>1247</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                // CPU sequential FFN</pre></td></tr><tr><td class='line-number'><a name='L1248' href='#L1248'><pre>1248</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                for <span class='region red'>hidden</span> in <span class='region red'>&amp;mut hidden_states</span> {</pre></td></tr><tr><td class='line-number'><a name='L1249' href='#L1249'><pre>1249</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>mut ffn_hidden</span> = <span class='region red'>self.model</span>.<span class='region red'>fused_matmul</span>(<span class='region red'>hidden</span>, <span class='region red'>&amp;layer.ffn_up_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1250' href='#L1250'><pre>1250</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>layer.ffn_up_bias</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1251' href='#L1251'><pre>1251</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>self.model</span><span class='region red'>.</span><span class='region red'>add_bias</span><span class='region red'>(&amp;mut ffn_hidden, bias);</span></pre></td></tr><tr><td class='line-number'><a name='L1252' href='#L1252'><pre>1252</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1253' href='#L1253'><pre>1253</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model</span>.<span class='region red'>gelu</span>(<span class='region red'>&amp;mut ffn_hidden</span>);</pre></td></tr><tr><td class='line-number'><a name='L1254' href='#L1254'><pre>1254</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1255' href='#L1255'><pre>1255</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>mut ffn_output</span> = <span class='region red'>self</span></pre></td></tr><tr><td class='line-number'><a name='L1256' href='#L1256'><pre>1256</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        .model</span></pre></td></tr><tr><td class='line-number'><a name='L1257' href='#L1257'><pre>1257</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        .<span class='region red'>fused_matmul</span>(<span class='region red'>&amp;ffn_hidden</span>, <span class='region red'>&amp;layer.ffn_down_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1258' href='#L1258'><pre>1258</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>layer.ffn_down_bias</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1259' href='#L1259'><pre>1259</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>self.model</span><span class='region red'>.</span><span class='region red'>add_bias</span><span class='region red'>(&amp;mut ffn_output, bias);</span></pre></td></tr><tr><td class='line-number'><a name='L1260' href='#L1260'><pre>1260</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1261' href='#L1261'><pre>1261</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1262' href='#L1262'><pre>1262</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    // Residual</pre></td></tr><tr><td class='line-number'><a name='L1263' href='#L1263'><pre>1263</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    for <span class='region red'>i</span> in 0..<span class='region red'>hidden_dim</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1264' href='#L1264'><pre>1264</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                        </span><span class='region red'>hidden[i]</span><span class='region red'> += ffn_output[i];</span></pre></td></tr><tr><td class='line-number'><a name='L1265' href='#L1265'><pre>1265</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    }</span></pre></td></tr><tr><td class='line-number'><a name='L1266' href='#L1266'><pre>1266</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                }</pre></td></tr><tr><td class='line-number'><a name='L1267' href='#L1267'><pre>1267</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L1268' href='#L1268'><pre>1268</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L1269' href='#L1269'><pre>1269</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1270' href='#L1270'><pre>1270</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-100: Parallel cache advance</pre></td></tr><tr><td class='line-number'><a name='L1271' href='#L1271'><pre>1271</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>caches</span>.<span class='region red'>par_iter_mut</span>().<span class='region red'>for_each</span>(|cache| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1272' href='#L1272'><pre>1272</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>cache</span>.<span class='region red'>advance</span>();</pre></td></tr><tr><td class='line-number'><a name='L1273' href='#L1273'><pre>1273</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>}</span>);</pre></td></tr><tr><td class='line-number'><a name='L1274' href='#L1274'><pre>1274</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1275' href='#L1275'><pre>1275</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // 3. Final layer norm and LM head for each prompt</pre></td></tr><tr><td class='line-number'><a name='L1276' href='#L1276'><pre>1276</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        // PARITY-025: Use GPU batch LM head when batch &gt;= threshold</pre></td></tr><tr><td class='line-number'><a name='L1277' href='#L1277'><pre>1277</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>vocab_size</span> = <span class='region red'>self.model.config.vocab_size</span>;</pre></td></tr><tr><td class='line-number'><a name='L1278' href='#L1278'><pre>1278</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1279' href='#L1279'><pre>1279</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>all_logits</span>: <span class='region red'>Vec&lt;Vec&lt;f32&gt;&gt;</span> = if <span class='region red'>use_gpu</span> {</pre></td></tr><tr><td class='line-number'><a name='L1280' href='#L1280'><pre>1280</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // GPU path: batch layer norm and LM head projection</pre></td></tr><tr><td class='line-number'><a name='L1281' href='#L1281'><pre>1281</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1282' href='#L1282'><pre>1282</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // 3a. PARITY-098: Parallel final layer norm</pre></td></tr><tr><td class='line-number'><a name='L1283' href='#L1283'><pre>1283</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>normed_batch</span>: <span class='region red'>Vec&lt;Vec&lt;f32&gt;&gt;</span> = <span class='region red'>hidden_states</span></pre></td></tr><tr><td class='line-number'><a name='L1284' href='#L1284'><pre>1284</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>par_iter</span>()</pre></td></tr><tr><td class='line-number'><a name='L1285' href='#L1285'><pre>1285</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>map</span>(|hidden| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1286' href='#L1286'><pre>1286</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model</span>.<span class='region red'>layer_norm</span>(</pre></td></tr><tr><td class='line-number'><a name='L1287' href='#L1287'><pre>1287</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>hidden</span>,</pre></td></tr><tr><td class='line-number'><a name='L1288' href='#L1288'><pre>1288</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>&amp;self.model.output_norm_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L1289' href='#L1289'><pre>1289</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model.output_norm_bias</span>.<span class='region red'>as_deref</span>(),</pre></td></tr><tr><td class='line-number'><a name='L1290' href='#L1290'><pre>1290</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                        <span class='region red'>self.model.config.eps</span>,</pre></td></tr><tr><td class='line-number'><a name='L1291' href='#L1291'><pre>1291</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                    )</pre></td></tr><tr><td class='line-number'><a name='L1292' href='#L1292'><pre>1292</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>)</pre></td></tr><tr><td class='line-number'><a name='L1293' href='#L1293'><pre>1293</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1294' href='#L1294'><pre>1294</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1295' href='#L1295'><pre>1295</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // 3b. Batch LM head projection using GPU GEMM (PARITY-025)</pre></td></tr><tr><td class='line-number'><a name='L1296' href='#L1296'><pre>1296</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>batch_normed</span>: <span class='region red'>Vec&lt;f32&gt;</span> = <span class='region red'>normed_batch.iter()</span>.<span class='region red'>flatten</span>().<span class='region red'>copied</span>().<span class='region red'>collect</span>();</pre></td></tr><tr><td class='line-number'><a name='L1297' href='#L1297'><pre>1297</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>batch_logits</span> = <span class='region red'>self</span>.<span class='region red'>batch_lm_head_gpu</span>(<span class='region red'>&amp;batch_normed</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1298' href='#L1298'><pre>1298</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1299' href='#L1299'><pre>1299</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // 3c. PARITY-098: Parallel scatter logits back to per-prompt vectors</pre></td></tr><tr><td class='line-number'><a name='L1300' href='#L1300'><pre>1300</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>(0..batch_size)</span></pre></td></tr><tr><td class='line-number'><a name='L1301' href='#L1301'><pre>1301</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>into_par_iter</span>()</pre></td></tr><tr><td class='line-number'><a name='L1302' href='#L1302'><pre>1302</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>map</span>(|i| <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1303' href='#L1303'><pre>1303</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    let <span class='region red'>start</span> = <span class='region red'>i * vocab_size</span>;</pre></td></tr><tr><td class='line-number'><a name='L1304' href='#L1304'><pre>1304</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>batch_logits</span><span class='region red'>[</span><span class='region red'>start</span><span class='region red'>..start + vocab_size]</span>.<span class='region red'>to_vec</span>()</pre></td></tr><tr><td class='line-number'><a name='L1305' href='#L1305'><pre>1305</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>}</span>)</pre></td></tr><tr><td class='line-number'><a name='L1306' href='#L1306'><pre>1306</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                .<span class='region red'>collect</span>()</pre></td></tr><tr><td class='line-number'><a name='L1307' href='#L1307'><pre>1307</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        } else {</pre></td></tr><tr><td class='line-number'><a name='L1308' href='#L1308'><pre>1308</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            // CPU path: sequential per-prompt processing</pre></td></tr><tr><td class='line-number'><a name='L1309' href='#L1309'><pre>1309</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            let <span class='region red'>mut result</span> = <span class='region red'>Vec::with_capacity</span>(<span class='region red'>batch_size</span>);</pre></td></tr><tr><td class='line-number'><a name='L1310' href='#L1310'><pre>1310</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            for <span class='region red'>hidden</span> in <span class='region red'>&amp;hidden_states</span> {</pre></td></tr><tr><td class='line-number'><a name='L1311' href='#L1311'><pre>1311</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>normed</span> = <span class='region red'>self.model</span>.<span class='region red'>layer_norm</span>(</pre></td></tr><tr><td class='line-number'><a name='L1312' href='#L1312'><pre>1312</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>hidden</span>,</pre></td></tr><tr><td class='line-number'><a name='L1313' href='#L1313'><pre>1313</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>&amp;self.model.output_norm_weight</span>,</pre></td></tr><tr><td class='line-number'><a name='L1314' href='#L1314'><pre>1314</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model.output_norm_bias</span>.<span class='region red'>as_deref</span>(),</pre></td></tr><tr><td class='line-number'><a name='L1315' href='#L1315'><pre>1315</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    <span class='region red'>self.model.config.eps</span>,</pre></td></tr><tr><td class='line-number'><a name='L1316' href='#L1316'><pre>1316</pre></a></td><td class='skipped-line'></td><td class='code'><pre>                );</pre></td></tr><tr><td class='line-number'><a name='L1317' href='#L1317'><pre>1317</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1318' href='#L1318'><pre>1318</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                let <span class='region red'>mut logits</span> = <span class='region red'>self</span></pre></td></tr><tr><td class='line-number'><a name='L1319' href='#L1319'><pre>1319</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    .model</span></pre></td></tr><tr><td class='line-number'><a name='L1320' href='#L1320'><pre>1320</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                    .<span class='region red'>fused_matmul</span>(<span class='region red'>&amp;normed</span>, <span class='region red'>&amp;self.model.lm_head_weight</span>)<span class='region red'>?</span>;</pre></td></tr><tr><td class='line-number'><a name='L1321' href='#L1321'><pre>1321</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                if let Some(<span class='region red'>ref bias</span>) = <span class='region red'>self.model.lm_head_bias</span> <span class='region red'>{</span></pre></td></tr><tr><td class='line-number'><a name='L1322' href='#L1322'><pre>1322</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                    </span><span class='region red'>self.model</span><span class='region red'>.</span><span class='region red'>add_bias</span><span class='region red'>(&amp;mut logits, bias);</span></pre></td></tr><tr><td class='line-number'><a name='L1323' href='#L1323'><pre>1323</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>                </span><span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1324' href='#L1324'><pre>1324</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>                <span class='region red'>result</span>.<span class='region red'>push</span>(<span class='region red'>logits</span>);</pre></td></tr><tr><td class='line-number'><a name='L1325' href='#L1325'><pre>1325</pre></a></td><td class='skipped-line'></td><td class='code'><pre>            }</pre></td></tr><tr><td class='line-number'><a name='L1326' href='#L1326'><pre>1326</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>            <span class='region red'>result</span></pre></td></tr><tr><td class='line-number'><a name='L1327' href='#L1327'><pre>1327</pre></a></td><td class='skipped-line'></td><td class='code'><pre>        };</pre></td></tr><tr><td class='line-number'><a name='L1328' href='#L1328'><pre>1328</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1329' href='#L1329'><pre>1329</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>Ok(all_logits)</span></pre></td></tr><tr><td class='line-number'><a name='L1330' href='#L1330'><pre>1330</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1331' href='#L1331'><pre>1331</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1332' href='#L1332'><pre>1332</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Get batch generation statistics</pre></td></tr><tr><td class='line-number'><a name='L1333' href='#L1333'><pre>1333</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    ///</pre></td></tr><tr><td class='line-number'><a name='L1334' href='#L1334'><pre>1334</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    /// Returns information about the batch processing capabilities.</pre></td></tr><tr><td class='line-number'><a name='L1335' href='#L1335'><pre>1335</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>pub fn batch_stats(&amp;self) -&gt; BatchGenerationStats</span> {</pre></td></tr><tr><td class='line-number'><a name='L1336' href='#L1336'><pre>1336</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>is_cached</span> = <span class='region red'>self</span>.<span class='region red'>is_gpu_cache_warm</span>();</pre></td></tr><tr><td class='line-number'><a name='L1337' href='#L1337'><pre>1337</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>memory_gb</span> = <span class='region red'>self.gpu_cache_memory() as f64 / 1_000_000_000.0</span>;</pre></td></tr><tr><td class='line-number'><a name='L1338' href='#L1338'><pre>1338</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>num_layers</span> = <span class='region red'>self.model.layers</span>.<span class='region red'>len</span>();</pre></td></tr><tr><td class='line-number'><a name='L1339' href='#L1339'><pre>1339</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>hidden_dim</span> = <span class='region red'>self.model.config.hidden_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1340' href='#L1340'><pre>1340</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        let <span class='region red'>intermediate_dim</span> = <span class='region red'>self.model.config.intermediate_dim</span>;</pre></td></tr><tr><td class='line-number'><a name='L1341' href='#L1341'><pre>1341</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L1342' href='#L1342'><pre>1342</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>BatchGenerationStats {</span></pre></td></tr><tr><td class='line-number'><a name='L1343' href='#L1343'><pre>1343</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            gpu_cache_ready: </span><span class='region red'>is_cached</span><span class='region red'>,</span></pre></td></tr><tr><td class='line-number'><a name='L1344' href='#L1344'><pre>1344</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            cache_memory_gb: </span><span class='region red'>memory_gb</span><span class='region red'>,</span></pre></td></tr><tr><td class='line-number'><a name='L1345' href='#L1345'><pre>1345</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            </span><span class='region red'>num_layers</span><span class='region red'>,</span></pre></td></tr><tr><td class='line-number'><a name='L1346' href='#L1346'><pre>1346</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            hidden_dim,</span></pre></td></tr><tr><td class='line-number'><a name='L1347' href='#L1347'><pre>1347</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            intermediate_dim,</span></pre></td></tr><tr><td class='line-number'><a name='L1348' href='#L1348'><pre>1348</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            recommended_batch_size: 32, // GPU GEMM threshold</span></pre></td></tr><tr><td class='line-number'><a name='L1349' href='#L1349'><pre>1349</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>            max_batch_size: 64,         // Memory-limited</span></pre></td></tr><tr><td class='line-number'><a name='L1350' href='#L1350'><pre>1350</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre><span class='region red'>        }</span></pre></td></tr><tr><td class='line-number'><a name='L1351' href='#L1351'><pre>1351</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    <span class='region red'>}</span></pre></td></tr><tr><td class='line-number'><a name='L1352' href='#L1352'><pre>1352</pre></a></td><td class='skipped-line'></td><td class='code'><pre>}</pre></td></tr></table></div></body></html>